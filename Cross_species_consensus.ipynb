{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f227ed1a",
   "metadata": {},
   "source": [
    "# Cross-Species Consensus Peak Analysis\n",
    "\n",
    "This notebook creates a unified consensus peak set across all species for comparative ATAC-seq analysis.\n",
    "\n",
    "## Workflow\n",
    "1. **Liftover** species consensus peaks ‚Üí hg38\n",
    "2. **Merge** all lifted peaks + Human peaks into unified hg38 consensus\n",
    "3. **Add peak IDs** for cross-species tracking\n",
    "4. **Liftback** unified peaks to each species genome\n",
    "5. **Create peak matrix** showing presence/absence across species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d188467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Add atac_pipeline to path\n",
    "sys.path.insert(0, '/cluster/home/jjanssens/jjans/analysis/adult_intestine/peaks/peak_calling/atac_pipeline')\n",
    "\n",
    "from src.liftover import liftover_peaks, liftover_two_step, get_chain_file, CHAIN_FILES\n",
    "from src.cross_species import (\n",
    "    cross_species_consensus_pipeline,\n",
    "    merge_bed_files,\n",
    "    add_peak_ids,\n",
    "    liftback_peaks,\n",
    "    create_peak_matrix,\n",
    "    get_reverse_chain_file,\n",
    "    REVERSE_CHAIN_FILES,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ atac_pipeline loaded successfully\")\n",
    "print(f\"   Available chain files (species ‚Üí hg38): {list(CHAIN_FILES.keys())}\")\n",
    "print(f\"   Available reverse chains (hg38 ‚Üí species): {list(REVERSE_CHAIN_FILES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eef9877",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a548289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "# Base paths\n",
    "BASE_PATH = \"/cluster/project/treutlein/USERS/jjans\"\n",
    "CHAIN_DIR = \"/cluster/work/treutlein/jjans/data/intestine/nhp_atlas/genomes/chain_files\"\n",
    "\n",
    "# Output directory for cross-species analysis\n",
    "OUTPUT_DIR = f\"{BASE_PATH}/analysis/adult_intestine/peaks/cross_species_consensus\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Species and their consensus peak files\n",
    "SPECIES_LIST = [\"Bonobo\", \"Chimpanzee\", \"Gorilla\", \"Macaque\", \"Marmoset\", \"Human\"]\n",
    "\n",
    "SPECIES_BEDS = {}\n",
    "for species in SPECIES_LIST:\n",
    "    bed_file = f\"{BASE_PATH}/analysis/adult_intestine/peaks/consensus_peak_calling_{species}/Consensus_Peaks_Filtered_500.bed\"\n",
    "    if os.path.exists(bed_file):\n",
    "        SPECIES_BEDS[species] = bed_file\n",
    "        print(f\"‚úÖ {species}: {bed_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {species}: NOT FOUND - {bed_file}\")\n",
    "\n",
    "print(f\"\\nüì¶ Found {len(SPECIES_BEDS)} species with consensus peaks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394be786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check peak counts per species\n",
    "print(\"üìä Peak counts per species:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_peaks = 0\n",
    "for species, bed_file in SPECIES_BEDS.items():\n",
    "    with open(bed_file) as f:\n",
    "        count = sum(1 for line in f if line.strip() and not line.startswith('#'))\n",
    "    total_peaks += count\n",
    "    print(f\"   {species:<15} {count:>10,} peaks\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"   {'TOTAL':<15} {total_peaks:>10,} peaks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c447e",
   "metadata": {},
   "source": [
    "## Step 1: Load Pre-Lifted Peaks (already lifted to hg38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c40cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-lifted peaks (already lifted to hg38)\n",
    "LIFTED_PEAKS_DIR = f\"{BASE_PATH}/analysis/adult_intestine/peaks/lifted_consensus_peaks\"\n",
    "\n",
    "# Map species to their lifted hg38 files\n",
    "LIFTED_BEDS = {\n",
    "    \"Bonobo\": f\"{LIFTED_PEAKS_DIR}/Consensus_Peaks_Filtered_500.hg38_Bonobo.bed\",\n",
    "    \"Chimpanzee\": f\"{LIFTED_PEAKS_DIR}/Consensus_Peaks_Filtered_500.hg38_Chimpanzee.bed\",\n",
    "    \"Gorilla\": f\"{LIFTED_PEAKS_DIR}/Consensus_Peaks_Filtered_500.hg38_Gorilla.bed\",\n",
    "    \"Macaque\": f\"{LIFTED_PEAKS_DIR}/Consensus_Peaks_Filtered_500.hg38_Macaque.bed\",\n",
    "    \"Marmoset\": f\"{LIFTED_PEAKS_DIR}/Consensus_Peaks_Filtered_500.hg38_Marmoset.bed\",\n",
    "    \"Human\": f\"{BASE_PATH}/analysis/adult_intestine/peaks/consensus_peak_calling_Human/Consensus_Peaks_Filtered_500.bed\",\n",
    "}\n",
    "\n",
    "# Check which files exist and count peaks\n",
    "print(\"üìÇ Loading pre-lifted peaks (hg38 coordinates):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "lift_results = {}\n",
    "for species, bed_file in LIFTED_BEDS.items():\n",
    "    if os.path.exists(bed_file):\n",
    "        with open(bed_file) as f:\n",
    "            count = sum(1 for line in f if line.strip() and not line.startswith('#'))\n",
    "        \n",
    "        lift_results[species] = {\n",
    "            \"status\": \"success\",\n",
    "            \"lifted\": count,\n",
    "            \"unmapped\": 0,\n",
    "            \"output_file\": bed_file\n",
    "        }\n",
    "        print(f\"   ‚úÖ {species:<15} {count:>10,} peaks  ({bed_file})\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {species:<15} NOT FOUND: {bed_file}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"üì¶ Loaded {len(lift_results)} species with lifted peaks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b54bd9",
   "metadata": {},
   "source": [
    "## Step 2: Merge All Peaks into Unified hg38 Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all successfully loaded lifted BED files\n",
    "lifted_beds = []\n",
    "for species in SPECIES_LIST:\n",
    "    if species in lift_results and lift_results[species][\"status\"] == \"success\":\n",
    "        bed_file = lift_results[species][\"output_file\"]\n",
    "        if os.path.exists(bed_file):\n",
    "            lifted_beds.append(bed_file)\n",
    "            print(f\"‚úÖ Including: {species}\")\n",
    "\n",
    "print(f\"\\nüì¶ Total files to merge: {len(lifted_beds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all lifted peaks into unified consensus\n",
    "MERGED_DIR = os.path.join(OUTPUT_DIR, \"02_merged_consensus\")\n",
    "os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "merged_bed = os.path.join(MERGED_DIR, \"unified_consensus_hg38_merged.bed\")\n",
    "\n",
    "merge_result = merge_bed_files(\n",
    "    input_beds=lifted_beds,\n",
    "    output_bed=merged_bed,\n",
    "    merge_distance=0,  # Only merge overlapping peaks\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n{merge_result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04dad3",
   "metadata": {},
   "source": [
    "## Step 3: Add Peak IDs for Cross-Species Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unique peak IDs to the merged consensus\n",
    "unified_with_ids = os.path.join(MERGED_DIR, \"unified_consensus_hg38_with_ids.bed\")\n",
    "\n",
    "id_result = add_peak_ids(\n",
    "    input_bed=merged_bed,\n",
    "    output_bed=unified_with_ids,\n",
    "    prefix=\"unified\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Show first few peaks\n",
    "print(\"\\nüìÑ First 10 unified peaks:\")\n",
    "df_unified = pd.read_csv(unified_with_ids, sep='\\t', header=None, \n",
    "                         names=['Chromosome', 'Start', 'End', 'PeakID'])\n",
    "print(df_unified.head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef0078",
   "metadata": {},
   "source": [
    "## Step 4: Liftback Unified Peaks to Each Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liftback to each species with species-specific match rates\n",
    "# Using 80% of forward liftover rates for liftback (slightly more permissive for recovery)\n",
    "\n",
    "LIFTBACK_DIR = os.path.join(OUTPUT_DIR, \"03_lifted_back\")\n",
    "os.makedirs(LIFTBACK_DIR, exist_ok=True)\n",
    "\n",
    "# Forward rates used during species ‚Üí hg38 liftover\n",
    "FORWARD_MATCH_RATES = {\n",
    "    \"Bonobo\": 0.9,\n",
    "    \"Chimpanzee\": 0.9,\n",
    "    \"Gorilla\": 0.9,\n",
    "    \"Macaque\": 0.8,\n",
    "    \"Marmoset\": 0.6,\n",
    "    \"Human\": 1.0,  # Not used\n",
    "}\n",
    "\n",
    "# Liftback rates = 80% of forward rates\n",
    "LIFTBACK_MATCH_RATES = {species: rate * 0.8 for species, rate in FORWARD_MATCH_RATES.items()}\n",
    "\n",
    "print(\"üìä Liftback min_match rates (80% of forward rates):\")\n",
    "for species, rate in LIFTBACK_MATCH_RATES.items():\n",
    "    if species != \"Human\":\n",
    "        print(f\"   {species}: {rate:.2f}\")\n",
    "\n",
    "liftback_results = {}\n",
    "\n",
    "for species in SPECIES_LIST:\n",
    "    output_bed = os.path.join(LIFTBACK_DIR, f\"unified_consensus_{species}.bed\")\n",
    "    \n",
    "    if species == \"Human\":\n",
    "        # Human stays as is (already in hg38)\n",
    "        print(f\"\\nüß¨ {species}: Copying hg38 consensus (no liftover needed)\")\n",
    "        \n",
    "        import shutil\n",
    "        shutil.copy(unified_with_ids, output_bed)\n",
    "        \n",
    "        with open(output_bed) as f:\n",
    "            count = sum(1 for _ in f)\n",
    "        \n",
    "        liftback_results[species] = {\n",
    "            \"status\": \"success\",\n",
    "            \"lifted\": count,\n",
    "            \"unmapped\": 0,\n",
    "            \"output_file\": output_bed\n",
    "        }\n",
    "        print(f\"   ‚úÖ {count:,} peaks\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüîô Lifting back to {species} (min_match={LIFTBACK_MATCH_RATES[species]:.2f})...\")\n",
    "    \n",
    "    result = liftback_peaks(\n",
    "        input_bed=unified_with_ids,\n",
    "        output_bed=output_bed,\n",
    "        species=species,\n",
    "        chain_dir=CHAIN_DIR,\n",
    "        min_match=LIFTBACK_MATCH_RATES[species],\n",
    "        auto_chr=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    liftback_results[species] = result\n",
    "    print(f\"   {result['message']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c42f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of liftback\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LIFTBACK TO SPECIES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_unified = id_result[\"peak_count\"]\n",
    "print(f\"Unified hg38 peaks: {total_unified:,}\\n\")\n",
    "\n",
    "print(f\"{'Species':<15} {'Lifted':>10} {'Unmapped':>10} {'Rate':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for species in SPECIES_LIST:\n",
    "    if species in liftback_results:\n",
    "        r = liftback_results[species]\n",
    "        lifted = r.get(\"lifted\", 0)\n",
    "        unmapped = r.get(\"unmapped\", r.get(\"total_unmapped\", 0))\n",
    "        rate = (lifted / total_unified * 100) if total_unified > 0 else 0\n",
    "        print(f\"{species:<15} {lifted:>10,} {unmapped:>10,} {rate:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946f933",
   "metadata": {},
   "source": [
    "## Step 5: Create Peak Presence/Absence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49816c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build species beds dictionary for matrix creation\n",
    "species_liftback_beds = {}\n",
    "for species in SPECIES_LIST:\n",
    "    bed_file = os.path.join(LIFTBACK_DIR, f\"unified_consensus_{species}.bed\")\n",
    "    if os.path.exists(bed_file):\n",
    "        species_liftback_beds[species] = bed_file\n",
    "\n",
    "# Create peak matrix\n",
    "matrix_file = os.path.join(OUTPUT_DIR, \"peak_presence_matrix.tsv\")\n",
    "\n",
    "matrix_result = create_peak_matrix(\n",
    "    unified_human_bed=unified_with_ids,\n",
    "    species_beds=species_liftback_beds,\n",
    "    output_file=matrix_file,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a15f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize the matrix\n",
    "df_matrix = pd.read_csv(matrix_file, sep='\\t')\n",
    "print(f\"üìä Peak matrix shape: {df_matrix.shape}\")\n",
    "print(df_matrix.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b198bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate conservation statistics\n",
    "species_cols = [col for col in df_matrix.columns if col in SPECIES_LIST]\n",
    "\n",
    "# Number of species each peak is present in\n",
    "df_matrix['n_species'] = df_matrix[species_cols].sum(axis=1)\n",
    "\n",
    "# Conservation categories\n",
    "conservation_counts = df_matrix['n_species'].value_counts().sort_index()\n",
    "\n",
    "print(\"üìä Peak Conservation:\")\n",
    "print(\"-\" * 40)\n",
    "for n, count in conservation_counts.items():\n",
    "    pct = count / len(df_matrix) * 100\n",
    "    print(f\"   Present in {n} species: {count:>8,} ({pct:>5.1f}%)\")\n",
    "\n",
    "# Summary stats\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"   Total unified peaks: {len(df_matrix):,}\")\n",
    "print(f\"   Conserved in ALL {len(species_cols)} species: {(df_matrix['n_species'] == len(species_cols)).sum():,}\")\n",
    "print(f\"   Species-specific (1 species only): {(df_matrix['n_species'] == 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize conservation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of conservation levels\n",
    "ax1 = axes[0]\n",
    "conservation_counts.plot(kind='bar', ax=ax1, color='steelblue', edgecolor='black')\n",
    "ax1.set_xlabel('Number of Species')\n",
    "ax1.set_ylabel('Number of Peaks')\n",
    "ax1.set_title('Peak Conservation Across Species')\n",
    "ax1.set_xticklabels([f'{int(x)}' for x in conservation_counts.index], rotation=0)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, val) in enumerate(conservation_counts.items()):\n",
    "    ax1.text(i, val + 1000, f'{val/len(df_matrix)*100:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "# Heatmap of per-species presence\n",
    "ax2 = axes[1]\n",
    "species_presence = df_matrix[species_cols].sum() / len(df_matrix) * 100\n",
    "species_presence = species_presence.sort_values(ascending=True)\n",
    "\n",
    "colors = plt.cm.Blues(species_presence / 100)\n",
    "bars = ax2.barh(species_presence.index, species_presence.values, color=colors, edgecolor='black')\n",
    "ax2.set_xlabel('% of Unified Peaks Present')\n",
    "ax2.set_title('Peak Presence by Species')\n",
    "ax2.set_xlim(0, 100)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (species, val) in enumerate(species_presence.items()):\n",
    "    ax2.text(val + 1, i, f'{val:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"conservation_summary.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73de9dfd",
   "metadata": {},
   "source": [
    "## Summary: Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cde935",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CROSS-SPECIES CONSENSUS PIPELINE - OUTPUT FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìÅ Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "print(f\"\\nüìÑ Main outputs:\")\n",
    "print(f\"   Unified hg38 consensus: {unified_with_ids}\")\n",
    "print(f\"   Peak presence matrix: {matrix_file}\")\n",
    "\n",
    "print(f\"\\nüìÑ Liftback files (peaks in each species' genome):\")\n",
    "for species in SPECIES_LIST:\n",
    "    bed_file = os.path.join(LIFTBACK_DIR, f\"unified_consensus_{species}.bed\")\n",
    "    if os.path.exists(bed_file):\n",
    "        with open(bed_file) as f:\n",
    "            count = sum(1 for _ in f)\n",
    "        print(f\"   {species}: {bed_file} ({count:,} peaks)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline complete!\")\n",
    "print(f\"   Total unified peaks: {id_result['peak_count']:,}\")\n",
    "print(f\"   Use the peak_id (column 4) to compare accessibility across species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4aa53",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Quantification Over Unified Peaks\n",
    "\n",
    "Quantify fragment files, bigwigs, or Tn5 insertions over the unified peak set.\n",
    "- Supports **coverage** (fragment overlap) or **cut-sites** (Tn5 insertion) counting\n",
    "- Parallel processing by file or by region chunks\n",
    "- Memory-efficient streaming for very large matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import quantification functions\n",
    "from src.quantification import quantify, quantify_matrix, save_matrix, load_matrix\n",
    "\n",
    "print(\"‚úÖ Quantification module loaded\")\n",
    "print(\"   quantify()        ‚Äì single file over peaks\")\n",
    "print(\"   quantify_matrix() ‚Äì multiple files ‚Üí peaks √ó samples matrix\")\n",
    "print(\"   save/load_matrix  ‚Äì feather / parquet / tsv I/O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa225f",
   "metadata": {},
   "source": [
    "### Configure Input Files\n",
    "\n",
    "Set up paths and discover input files. All three input types (fragments, bigwig, tn5) use the same `quantify()` / `quantify_matrix()` functions ‚Äî just change `input_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7877a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION: Adjust these paths ===\n",
    "\n",
    "SPECIES = \"Human\"  # Or loop over species\n",
    "QUANT_OUTPUT_DIR = os.path.join(OUTPUT_DIR, \"04_quantification\")\n",
    "os.makedirs(QUANT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Peak file for quantification:\n",
    "# - For human: use unified_with_ids (already hg38)\n",
    "# - For other species: use the liftback file\n",
    "peak_file_for_quant = unified_with_ids  # Or: liftback_results[SPECIES][\"output_file\"]\n",
    "\n",
    "# Discover input files (update the directory and extension as needed)\n",
    "import glob\n",
    "\n",
    "INPUT_DIR = f\"{BASE_PATH}/analysis/adult_intestine/peaks/fragments_{SPECIES}\"\n",
    "\n",
    "input_files = sorted(\n",
    "    glob.glob(f\"{INPUT_DIR}/*.tsv.gz\")   # fragments\n",
    "    # glob.glob(f\"{INPUT_DIR}/*.bw\")     # bigwigs\n",
    "    # glob.glob(f\"{INPUT_DIR}/*.bed\")    # tn5 insertions\n",
    ")\n",
    "\n",
    "print(f\"Found {len(input_files)} input files in {INPUT_DIR}\")\n",
    "for f in input_files[:5]:\n",
    "    print(f\"   {os.path.basename(f)}\")\n",
    "if len(input_files) > 5:\n",
    "    print(f\"   ... and {len(input_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SINGLE FILE ===\n",
    "# Quantify one file over peaks (returns a pd.Series)\n",
    "# Useful for testing or when you only have one sample.\n",
    "\n",
    "if input_files:\n",
    "    result = quantify(\n",
    "        input_file=input_files[0],\n",
    "        peak_file=peak_file_for_quant,\n",
    "        input_type=\"fragments\",   # \"fragments\" | \"tn5\" | \"bigwig\"\n",
    "        method=\"cutsites\",        # \"coverage\" | \"cutsites\"  (fragments only)\n",
    "        # stat=\"mean\",            # \"mean\"|\"sum\"|\"max\"|\"min\" (bigwig only)\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f\"\\nüìä Result shape: {result.shape}\")\n",
    "    print(result.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No input files found. Update INPUT_DIR.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4519d8e",
   "metadata": {},
   "source": [
    "### Build Quantification Matrix (multiple files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d55da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MULTIPLE FILES ‚Üí MATRIX ===\n",
    "# Builds a peaks √ó samples matrix in parallel.\n",
    "# Works for fragments, tn5, or bigwig ‚Äî just change input_type.\n",
    "\n",
    "if input_files:\n",
    "    matrix = quantify_matrix(\n",
    "        input_files=input_files,\n",
    "        peak_file=peak_file_for_quant,\n",
    "        input_type=\"fragments\",       # \"fragments\" | \"tn5\" | \"bigwig\"\n",
    "        method=\"cutsites\",            # \"coverage\" | \"cutsites\"  (fragments only)\n",
    "        # stat=\"mean\",                # \"mean\"|\"sum\"|\"max\"|\"min\" (bigwig only)\n",
    "        n_workers=8,\n",
    "        name_pattern=r\"_fragments.*$\",  # regex to clean filenames\n",
    "        name_replacement=\"\",\n",
    "        # sample_names=[\"A\", \"B\", ...],  # or provide explicit names\n",
    "        output_file=os.path.join(QUANT_OUTPUT_DIR, f\"quantification_{SPECIES}\"),\n",
    "        output_format=\"feather\",       # \"feather\" | \"parquet\" | \"tsv\"\n",
    "        # chunk_size=50,               # memory-efficient: process N files at a time\n",
    "        verbose=True,\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No input files found. Update INPUT_DIR.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d851cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MEMORY-EFFICIENT MODE ===\n",
    "# For very large numbers of files, use chunk_size to avoid loading the\n",
    "# full matrix in memory.  Just add chunk_size= to quantify_matrix():\n",
    "\n",
    "# quantify_matrix(\n",
    "#     input_files=input_files,\n",
    "#     peak_file=peak_file_for_quant,\n",
    "#     input_type=\"fragments\",\n",
    "#     method=\"cutsites\",\n",
    "#     n_workers=8,\n",
    "#     chunk_size=50,          # ‚Üê process 50 files at a time, write chunks\n",
    "#     output_file=os.path.join(QUANT_OUTPUT_DIR, f\"quantification_large_{SPECIES}\"),\n",
    "#     output_format=\"parquet\",  # parquet recommended for large files\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "print(\"üí° Uncomment and adjust chunk_size for large-scale quantification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0efdc8e",
   "metadata": {},
   "source": [
    "### Load and Inspect a Saved Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927498a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved matrix (auto-detects feather / parquet / tsv from extension)\n",
    "# quant_df = load_matrix(os.path.join(QUANT_OUTPUT_DIR, f\"quantification_{SPECIES}.feather\"))\n",
    "# print(f\"üìä Matrix shape: {quant_df.shape}\")\n",
    "# print(f\"   Peaks:   {quant_df.shape[0]:,}\")\n",
    "# print(f\"   Samples: {quant_df.shape[1]}\")\n",
    "# print(quant_df.head())\n",
    "\n",
    "print(\"üí° Uncomment after running quantification to inspect results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
