{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9499c63",
   "metadata": {},
   "source": [
    "# Flexible ATAC-seq Peak Calling Workflow\n",
    "\n",
    "**üéØ Use this notebook for custom analyses with your own files.**\n",
    "\n",
    "This notebook provides full flexibility to:\n",
    "- Use your own fragment files from any location\n",
    "- Specify custom chain files for liftover\n",
    "- Set individual parameters for each step\n",
    "- Run only the steps you need\n",
    "\n",
    "For a more streamlined workflow using `config.yaml`, use `01_peak_calling_workflow.ipynb`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213952ab",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "PIPELINE_DIR = Path(os.getcwd()).parent if 'notebooks' in os.getcwd() else Path(os.getcwd())\n",
    "sys.path.insert(0, str(PIPELINE_DIR))\n",
    "\n",
    "# Import all pipeline modules\n",
    "from src.peak_calling import (\n",
    "    convert_fragments_to_cutsites,\n",
    "    process_all_fragments,\n",
    "    run_peak_calling,\n",
    "    EFFECTIVE_GENOME_SIZES,\n",
    "    DEFAULT_MACS3_PARAMS,\n",
    ")\n",
    "from src.consensus import (\n",
    "    get_consensus_peaks,\n",
    "    load_narrowpeaks,\n",
    "    harmonize_chromosomes,\n",
    ")\n",
    "from src.liftover import (\n",
    "    liftover_peaks,\n",
    "    print_chain_info,\n",
    "    get_chain_file,\n",
    "    CHAIN_FILES,\n",
    "    DEFAULT_CHAIN_DIR,\n",
    ")\n",
    "from src.bigwig import (\n",
    "    create_bigwig,\n",
    "    fragments_to_bigwig,\n",
    "    process_all_fragments_to_bigwig,\n",
    ")\n",
    "from src.utils import get_chromsizes, save_parameters, ensure_dir\n",
    "from src.visualization import plot_peak_distribution, plot_consensus_summary, plot_genome_regions\n",
    "\n",
    "print(f\"‚úÖ Pipeline loaded from: {PIPELINE_DIR}\")\n",
    "print(f\"‚úÖ Python: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137e068",
   "metadata": {},
   "source": [
    "## üìÇ Available Chain Files\n",
    "\n",
    "Chain files are required for liftover between genome assemblies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022998a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all available chain files\n",
    "print_chain_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef57dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List actual files in chain directory\n",
    "import subprocess\n",
    "print(\"\\nüìÅ Files in chain directory:\")\n",
    "print(f\"   {DEFAULT_CHAIN_DIR}\")\n",
    "print()\n",
    "\n",
    "if os.path.exists(DEFAULT_CHAIN_DIR):\n",
    "    for f in sorted(os.listdir(DEFAULT_CHAIN_DIR)):\n",
    "        if f.endswith('.chain') or f.endswith('.chain.gz'):\n",
    "            print(f\"   - {f}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Directory not accessible from this machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72875dd4",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Configuration: Set Your Paths\n",
    "\n",
    "**Edit the cells below to specify your input/output locations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e37aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ‚ö° USER CONFIGURATION - EDIT THESE VALUES\n",
    "# =============================================================================\n",
    "\n",
    "# --- Input Files ---\n",
    "# Path to your fragment file(s) - can be a single file or directory\n",
    "FRAGMENT_FILE = \"/path/to/your/fragments.tsv.gz\"  # Single file\n",
    "# OR\n",
    "FRAGMENT_DIR = \"/path/to/your/fragment_directory/\"  # Directory with multiple files\n",
    "\n",
    "# --- Species & Genome ---\n",
    "SPECIES = \"Gorilla\"  # Options: Human, Gorilla, Chimpanzee, Bonobo, Macaque, Marmoset\n",
    "\n",
    "# --- Reference Files ---\n",
    "# Chromosome sizes file\n",
    "CHROMSIZES_FILE = \"/path/to/your/genome.chrom.sizes\"\n",
    "\n",
    "# Chain file for liftover (leave empty if not doing liftover or Human)\n",
    "CHAIN_FILE = \"\"  # Will be auto-detected if empty\n",
    "\n",
    "# Chain file directory (default: Treutlein lab shared location)\n",
    "CHAIN_DIR = DEFAULT_CHAIN_DIR\n",
    "\n",
    "# --- Output Directory ---\n",
    "OUTPUT_DIR = str(PIPELINE_DIR / \"output\" / \"custom_analysis\")\n",
    "\n",
    "# --- MACS3 ---\n",
    "MACS3_PATH = \"macs3\"  # or full path like: \"/path/to/macs3\"\n",
    "\n",
    "print(\"Configuration set. Review the values above and modify as needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25057c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect chain file if not specified\n",
    "if not CHAIN_FILE and SPECIES != \"Human\":\n",
    "    CHAIN_FILE = get_chain_file(SPECIES, CHAIN_DIR)\n",
    "    print(f\"üîó Auto-detected chain file: {CHAIN_FILE}\")\n",
    "elif SPECIES == \"Human\":\n",
    "    print(\"‚ÑπÔ∏è Human samples don't need liftover\")\n",
    "else:\n",
    "    print(f\"üîó Using specified chain file: {CHAIN_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30143c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "CUTSITES_DIR = os.path.join(OUTPUT_DIR, \"cutsites\")\n",
    "PEAKS_DIR = os.path.join(OUTPUT_DIR, \"peaks\")\n",
    "LIFTED_DIR = os.path.join(OUTPUT_DIR, \"lifted\")\n",
    "CONSENSUS_DIR = os.path.join(OUTPUT_DIR, \"consensus\")\n",
    "BIGWIG_DIR = os.path.join(OUTPUT_DIR, \"bigwigs\")\n",
    "\n",
    "for d in [CUTSITES_DIR, PEAKS_DIR, LIFTED_DIR, CONSENSUS_DIR, BIGWIG_DIR]:\n",
    "    ensure_dir(d)\n",
    "\n",
    "print(f\"üìÅ Output directories created in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0766792",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Available Genome Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b09e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Effective genome sizes for MACS3:\")\n",
    "print(\"=\" * 40)\n",
    "for species, size in sorted(EFFECTIVE_GENOME_SIZES.items()):\n",
    "    marker = \"üëâ\" if species == SPECIES else \"  \"\n",
    "    print(f\"{marker} {species}: {size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d994d9",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Step 1: Fragment to Cut-site Conversion\n",
    "\n",
    "Convert fragment files to Tn5 cut-site BED files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Convert a single file\n",
    "def convert_single_file(input_file, output_dir):\n",
    "    \"\"\"Convert a single fragment file to cut-sites.\"\"\"\n",
    "    output_file = os.path.join(\n",
    "        output_dir,\n",
    "        os.path.basename(input_file).replace('.tsv.gz', '.cutsites.bed.gz')\n",
    "    )\n",
    "    \n",
    "    result = convert_fragments_to_cutsites(input_file, output_file)\n",
    "    print(f\"‚úÖ {result['message']}\")\n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# result = convert_single_file(FRAGMENT_FILE, CUTSITES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa88c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Convert all files in a directory\n",
    "def convert_all_files(input_dir, output_dir, workers=8):\n",
    "    \"\"\"Convert all fragment files in a directory.\"\"\"\n",
    "    results = process_all_fragments(\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir,\n",
    "        max_workers=workers,\n",
    "    )\n",
    "    \n",
    "    success = sum(1 for r in results if r['status'] == 'success')\n",
    "    print(f\"\\n‚úÖ Converted {success}/{len(results)} files\")\n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# results = convert_all_files(FRAGMENT_DIR, CUTSITES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a57a60",
   "metadata": {},
   "source": [
    "---\n",
    "## üèîÔ∏è Step 2: MACS3 Peak Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b2df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MACS3 Parameters (customize as needed)\n",
    "MACS3_PARAMS = {\n",
    "    'format': 'BED',\n",
    "    'qvalue': 0.01,      # FDR threshold\n",
    "    'shift': -73,        # ATAC-seq shift\n",
    "    'extsize': 146,      # Extension size\n",
    "    'keep_dup': 'all',\n",
    "    'min_length': 200,\n",
    "    'nomodel': True,\n",
    "    'call_summits': True,\n",
    "    'nolambda': True,\n",
    "}\n",
    "\n",
    "print(\"MACS3 Parameters:\")\n",
    "for k, v in MACS3_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f59aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run peak calling\n",
    "def call_peaks(input_dir, output_dir, species, workers=15):\n",
    "    \"\"\"Run MACS3 peak calling on cut-site files.\"\"\"\n",
    "    results = run_peak_calling(\n",
    "        species=species,\n",
    "        frag_dir=input_dir,\n",
    "        out_dir=output_dir,\n",
    "        macs3_path=MACS3_PATH,\n",
    "        max_workers=workers,\n",
    "        params=MACS3_PARAMS,\n",
    "    )\n",
    "    \n",
    "    total_peaks = sum(r.get('peak_count', 0) for r in results if r['status'] == 'success')\n",
    "    print(f\"\\nüìä Total peaks called: {total_peaks:,}\")\n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# peak_results = call_peaks(CUTSITES_DIR, PEAKS_DIR, SPECIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3cf3c",
   "metadata": {},
   "source": [
    "---\n",
    "## üîó Step 3: Liftover to hg38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c781ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liftover a single file\n",
    "def liftover_single(input_bed, output_bed, chain_file):\n",
    "    \"\"\"Liftover a single BED file.\"\"\"\n",
    "    result = liftover_peaks(\n",
    "        input_bed=input_bed,\n",
    "        output_bed=output_bed,\n",
    "        chain_file=chain_file,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# result = liftover_single(\n",
    "#     input_bed=\"/path/to/peaks.bed\",\n",
    "#     output_bed=\"/path/to/peaks.hg38.bed\",\n",
    "#     chain_file=CHAIN_FILE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9efe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liftover all narrowPeak files\n",
    "def liftover_all_peaks(peaks_dir, output_dir, chain_file):\n",
    "    \"\"\"Liftover all narrowPeak files in a directory.\"\"\"\n",
    "    peak_files = list(Path(peaks_dir).glob(\"*_peaks.narrowPeak\"))\n",
    "    print(f\"Found {len(peak_files)} peak files\")\n",
    "    print(f\"üîó Using chain file: {chain_file}\")\n",
    "    \n",
    "    results = []\n",
    "    for pf in peak_files:\n",
    "        out_file = Path(output_dir) / pf.name.replace(\".narrowPeak\", \".hg38.bed\")\n",
    "        result = liftover_peaks(\n",
    "            input_bed=str(pf),\n",
    "            output_bed=str(out_file),\n",
    "            chain_file=chain_file,\n",
    "            verbose=False,\n",
    "        )\n",
    "        print(f\"  {pf.name}: {result['lifted']:,} lifted, {result['unmapped']:,} unmapped\")\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# liftover_results = liftover_all_peaks(PEAKS_DIR, LIFTED_DIR, CHAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d409bc17",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Step 4: Consensus Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consensus parameters\n",
    "CONSENSUS_PARAMS = {\n",
    "    'peak_half_width': 250,      # Total width = 500bp\n",
    "    'q_value_threshold': 0.05,   # Filter peaks by q-value\n",
    "    'min_peaks_per_sample': 5000,  # Minimum peaks to include sample\n",
    "}\n",
    "\n",
    "print(\"Consensus Peak Parameters:\")\n",
    "for k, v in CONSENSUS_PARAMS.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbfb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate consensus peaks\n",
    "def generate_consensus(peaks_dir, output_dir, chromsizes_file, params=CONSENSUS_PARAMS):\n",
    "    \"\"\"Generate consensus peaks from narrowPeak files.\"\"\"\n",
    "    \n",
    "    # Load peaks\n",
    "    peaks_dict = load_narrowpeaks(\n",
    "        peak_dir=peaks_dir,\n",
    "        q_value_threshold=params['q_value_threshold'],\n",
    "        min_peaks_per_sample=params['min_peaks_per_sample'],\n",
    "    )\n",
    "    print(f\"Loaded {len(peaks_dict)} samples\")\n",
    "    \n",
    "    # Load chromsizes\n",
    "    chromsizes = get_chromsizes(SPECIES, chromsizes_file, as_pyranges=True)\n",
    "    \n",
    "    # Harmonize\n",
    "    peaks_dict, chromsizes = harmonize_chromosomes(peaks_dict, chromsizes)\n",
    "    \n",
    "    # Generate consensus\n",
    "    consensus = get_consensus_peaks(\n",
    "        narrow_peaks_dict=peaks_dict,\n",
    "        peak_half_width=params['peak_half_width'],\n",
    "        chromsizes=chromsizes,\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    output_file = os.path.join(output_dir, f\"consensus_peaks_{params['peak_half_width']*2}bp.bed\")\n",
    "    consensus.to_bed(output_file)\n",
    "    print(f\"\\n‚úÖ Saved {len(consensus):,} consensus peaks to: {output_file}\")\n",
    "    \n",
    "    return consensus\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# consensus_peaks = generate_consensus(PEAKS_DIR, CONSENSUS_DIR, CHROMSIZES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2bf65",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Step 5: BigWig Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825731e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single BigWig file\n",
    "def make_bigwig(fragments_file, output_file, chromsizes_file, cut_sites=True, normalize=True):\n",
    "    \"\"\"Create a bigWig from a fragment file.\"\"\"\n",
    "    result = create_bigwig(\n",
    "        fragments=fragments_file,\n",
    "        chromsizes=chromsizes_file,\n",
    "        output=output_file,\n",
    "        cut_sites=cut_sites,\n",
    "        normalize=normalize,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# result = make_bigwig(\n",
    "#     fragments_file=\"/path/to/fragments.tsv.gz\",\n",
    "#     output_file=\"/path/to/output.bw\",\n",
    "#     chromsizes_file=CHROMSIZES_FILE,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BigWigs for all files in a directory\n",
    "def make_all_bigwigs(input_dir, output_dir, chromsizes_file, pattern=\"*.tsv.gz\"):\n",
    "    \"\"\"Create bigWigs for all fragment files in a directory.\"\"\"\n",
    "    results = process_all_fragments_to_bigwig(\n",
    "        input_dir=input_dir,\n",
    "        output_dir=output_dir,\n",
    "        chrom_sizes_file=chromsizes_file,\n",
    "        pattern=pattern,\n",
    "        cut_sites=True,\n",
    "        normalize=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# bigwig_results = make_all_bigwigs(FRAGMENT_DIR, BIGWIG_DIR, CHROMSIZES_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b291247",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Quick Reference: One-Liner Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f00d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference for common operations\n",
    "print(\"\"\"\n",
    "üìã QUICK REFERENCE\n",
    "==================\n",
    "\n",
    "# Convert fragments to cut-sites:\n",
    "convert_fragments_to_cutsites(\"input.tsv.gz\", \"output.bed.gz\")\n",
    "\n",
    "# Run MACS3 on all files:\n",
    "run_peak_calling(\"Gorilla\", \"cutsites_dir\", \"peaks_dir\", max_workers=15)\n",
    "\n",
    "# Liftover to hg38:\n",
    "liftover_peaks(\"peaks.bed\", \"peaks.hg38.bed\", \"gorGor4ToHg38.over.chain\")\n",
    "\n",
    "# Generate consensus peaks:\n",
    "get_consensus_peaks(peaks_dict, peak_half_width=250, chromsizes=chromsizes)\n",
    "\n",
    "# Create bigWig:\n",
    "create_bigwig(\"fragments.tsv.gz\", \"chromsizes\", \"output.bw\")\n",
    "\n",
    "# Get chain file for species:\n",
    "get_chain_file(\"Gorilla\")  # Auto-uses default chain directory\n",
    "\n",
    "# Print chain file info:\n",
    "print_chain_info()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5803d50",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Check Your Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate your configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"CONFIGURATION CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = [\n",
    "    (\"Fragment file\", FRAGMENT_FILE, os.path.exists(FRAGMENT_FILE) if FRAGMENT_FILE != \"/path/to/your/fragments.tsv.gz\" else False),\n",
    "    (\"Fragment directory\", FRAGMENT_DIR, os.path.exists(FRAGMENT_DIR) if FRAGMENT_DIR != \"/path/to/your/fragment_directory/\" else False),\n",
    "    (\"Chromsizes file\", CHROMSIZES_FILE, os.path.exists(CHROMSIZES_FILE) if CHROMSIZES_FILE != \"/path/to/your/genome.chrom.sizes\" else False),\n",
    "    (\"Chain file\", CHAIN_FILE, os.path.exists(CHAIN_FILE) if CHAIN_FILE else True),\n",
    "    (\"Output directory\", OUTPUT_DIR, os.path.exists(OUTPUT_DIR)),\n",
    "]\n",
    "\n",
    "for name, path, exists in checks:\n",
    "    status = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "    print(f\"{status} {name}:\")\n",
    "    print(f\"   {path}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n‚ÑπÔ∏è Edit the USER CONFIGURATION cell above to set your paths.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
