{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c371fa",
   "metadata": {},
   "source": [
    "# Cross-Species Consensus Peak Pipeline (v2)\n",
    "\n",
    "**Goal:** Build a unified cross-species ATAC-seq consensus peak set across 6 primate species,\n",
    "with full provenance tracking and gene annotation.\n",
    "\n",
    "**Pipeline overview:**\n",
    "1. Lift all non-human species peaks to hg38\n",
    "2. Merge lifted peaks + human peaks into a unified consensus, tracking which species contributed each peak\n",
    "3. Identify **human-specific peaks** (human peaks not overlapping any non-human lifted peak)\n",
    "4. Lift unified consensus back to each species genome\n",
    "5. Identify **species-specific peaks** (original species peaks not covered by any liftback peak)\n",
    "6. Annotate all peaks with closest gene (from species-appropriate GTF) and distance\n",
    "\n",
    "**Outputs:**\n",
    "- `unified_peak_NNNNNN` -- consensus peaks in hg38 with species-detection annotation\n",
    "- `human_peak_NNNNNN` -- human-specific peaks (hg38) not liftable to any species\n",
    "- `{species}_peak_NNNNNN` -- per-species peaks in native coordinates, not liftable to hg38\n",
    "- `peak_annotation.tsv` -- master annotation: peak_id, type, species detected, closest gene, distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fe7dd",
   "metadata": {},
   "source": [
    "## 1. Load Packages and Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from src.cross_species import (\n",
    "    cross_species_consensus_pipeline,\n",
    "    merge_with_species_tracking,\n",
    "    find_human_specific_peaks,\n",
    "    find_species_specific_peaks,\n",
    "    create_peak_annotation,\n",
    "    extract_gene_bed_from_gtf,\n",
    "    annotate_with_closest_gene,\n",
    "    add_peak_ids,\n",
    "    DEFAULT_GTF_FILES,\n",
    "    REVERSE_CHAIN_FILES,\n",
    ")\n",
    "from src.liftover import DEFAULT_CHAIN_DIR\n",
    "\n",
    "print(\"All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration -- edit paths here\n",
    "# =============================================================================\n",
    "\n",
    "PEAKS_BASE = \"/cluster/project/treutlein/USERS/jjans/analysis/adult_intestine/peaks\"\n",
    "\n",
    "# Human consensus peaks (separate from non-human species)\n",
    "HUMAN_BED = f\"{PEAKS_BASE}/consensus_peak_calling_Human/Consensus_Peaks_Filtered_500.bed\"\n",
    "\n",
    "# Non-human species consensus peaks\n",
    "SPECIES_BEDS = {\n",
    "    \"Bonobo\":      f\"{PEAKS_BASE}/consensus_peak_calling_Bonobo/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Chimpanzee\":  f\"{PEAKS_BASE}/consensus_peak_calling_Chimpanzee/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Gorilla\":     f\"{PEAKS_BASE}/consensus_peak_calling_Gorilla/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Macaque\":     f\"{PEAKS_BASE}/consensus_peak_calling_Macaque/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Marmoset\":    f\"{PEAKS_BASE}/consensus_peak_calling_Marmoset/Consensus_Peaks_Filtered_500.bed\",\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"{PEAKS_BASE}/cross_species_consensus_v2\"\n",
    "\n",
    "# liftOver executable\n",
    "LIFTOVER_PATH = \"/cluster/project/treutlein/jjans/software/miniforge3/envs/genomes/bin/liftOver\"\n",
    "\n",
    "# Chain file directory\n",
    "CHAIN_DIR = DEFAULT_CHAIN_DIR\n",
    "\n",
    "# GTF files for gene annotation (use defaults from cross_species.py)\n",
    "GTF_FILES = DEFAULT_GTF_FILES.copy()\n",
    "\n",
    "# Pipeline parameters\n",
    "MIN_MATCH = 0.95    # Minimum match ratio for liftOver\n",
    "MERGE_DISTANCE = 0  # Max distance between peaks to merge (0 = must overlap)\n",
    "NCPU = 1            # Parallel workers for liftover\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Chain file dir:   {CHAIN_DIR}\")\n",
    "print(f\"liftOver binary:  {LIFTOVER_PATH}\")\n",
    "print(f\"Species:          {list(SPECIES_BEDS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fa36f",
   "metadata": {},
   "source": [
    "## 2. Validate Input Files\n",
    "\n",
    "Check that all input peak files, chain files, and GTF files exist before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e676c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Validate all input files exist\n",
    "# =============================================================================\n",
    "from src.liftover import CHAIN_FILES, get_chain_file\n",
    "\n",
    "all_ok = True\n",
    "\n",
    "# Check human BED\n",
    "print(\"--- Human peaks ---\")\n",
    "if os.path.exists(HUMAN_BED):\n",
    "    n = sum(1 for l in open(HUMAN_BED) if l.strip() and not l.startswith('#'))\n",
    "    print(f\"  OK  Human: {n:,} peaks\")\n",
    "else:\n",
    "    print(f\"  MISSING  {HUMAN_BED}\")\n",
    "    all_ok = False\n",
    "\n",
    "# Check non-human BED files\n",
    "print(\"\\n--- Non-human species peaks ---\")\n",
    "for species, bed in SPECIES_BEDS.items():\n",
    "    if os.path.exists(bed):\n",
    "        n = sum(1 for l in open(bed) if l.strip() and not l.startswith('#'))\n",
    "        print(f\"  OK  {species}: {n:,} peaks\")\n",
    "    else:\n",
    "        print(f\"  MISSING  {species}: {bed}\")\n",
    "        all_ok = False\n",
    "\n",
    "# Check chain files (forward: species -> hg38)\n",
    "print(\"\\n--- Forward chain files (species -> hg38) ---\")\n",
    "for species in SPECIES_BEDS:\n",
    "    if species == \"Marmoset\":\n",
    "        for step in [\"Marmoset_step1\", \"Marmoset_step2\"]:\n",
    "            path = os.path.join(CHAIN_DIR, CHAIN_FILES[step])\n",
    "            status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "            print(f\"  {status}  {step}: {CHAIN_FILES[step]}\")\n",
    "            if status == \"MISSING\":\n",
    "                all_ok = False\n",
    "    else:\n",
    "        path = os.path.join(CHAIN_DIR, CHAIN_FILES[species])\n",
    "        status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "        print(f\"  {status}  {species}: {CHAIN_FILES[species]}\")\n",
    "        if status == \"MISSING\":\n",
    "            all_ok = False\n",
    "\n",
    "# Check reverse chain files (hg38 -> species)\n",
    "print(\"\\n--- Reverse chain files (hg38 -> species) ---\")\n",
    "for key, chain in REVERSE_CHAIN_FILES.items():\n",
    "    path = os.path.join(CHAIN_DIR, chain)\n",
    "    status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "    print(f\"  {status}  {key}: {chain}\")\n",
    "    if status == \"MISSING\":\n",
    "        all_ok = False\n",
    "\n",
    "# Check GTF files\n",
    "print(\"\\n--- GTF files for gene annotation ---\")\n",
    "for species, gtf in GTF_FILES.items():\n",
    "    status = \"OK\" if os.path.exists(gtf) else \"MISSING\"\n",
    "    print(f\"  {status}  {species}: {os.path.basename(gtf)}\")\n",
    "    if status == \"MISSING\":\n",
    "        all_ok = False\n",
    "\n",
    "# Check liftOver binary\n",
    "print(f\"\\n--- liftOver binary ---\")\n",
    "status = \"OK\" if os.path.exists(LIFTOVER_PATH) else \"MISSING\"\n",
    "print(f\"  {status}  {LIFTOVER_PATH}\")\n",
    "if status == \"MISSING\":\n",
    "    all_ok = False\n",
    "\n",
    "print(f\"\\n{'All inputs validated successfully' if all_ok else 'WARNING: Some inputs are missing!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1307f",
   "metadata": {},
   "source": [
    "## 3. Run Cross-Species Consensus Pipeline\n",
    "\n",
    "This executes all 6 steps:\n",
    "1. **Lift to hg38** -- liftOver each non-human species to human genome\n",
    "2. **Merge with tracking** -- merge all species (incl. human) with bedtools, recording which species contributed\n",
    "3. **Human-specific** -- human peaks not overlapping any lifted non-human peak\n",
    "4. **Lift back** -- lift unified consensus back to each species' genome\n",
    "5. **Species-specific** -- original species peaks not covered by any liftback peak\n",
    "6. **Annotate** -- closest gene and distance for every peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503dbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run the full pipeline\n",
    "# =============================================================================\n",
    "results = cross_species_consensus_pipeline(\n",
    "    species_beds=SPECIES_BEDS,\n",
    "    human_bed=HUMAN_BED,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    chain_dir=CHAIN_DIR,\n",
    "    liftover_path=LIFTOVER_PATH,\n",
    "    min_match=MIN_MATCH,\n",
    "    merge_distance=MERGE_DISTANCE,\n",
    "    peak_prefix=\"unified\",\n",
    "    gtf_files=GTF_FILES,\n",
    "    verbose=True,\n",
    "    ncpu=NCPU,\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline status: {results['status']}\")\n",
    "print(f\"Message: {results['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fda3c",
   "metadata": {},
   "source": [
    "## 4. Inspect Output Files\n",
    "\n",
    "Check what was produced in each output subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# List all output files with sizes\n",
    "# =============================================================================\n",
    "import subprocess\n",
    "\n",
    "print(\"Output directory structure:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for dirpath, dirnames, filenames in sorted(os.walk(OUTPUT_DIR)):\n",
    "    level = dirpath.replace(OUTPUT_DIR, \"\").count(os.sep)\n",
    "    indent = \"  \" * level\n",
    "    reldir = os.path.relpath(dirpath, OUTPUT_DIR)\n",
    "    print(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "    subindent = \"  \" * (level + 1)\n",
    "    for f in sorted(filenames):\n",
    "        fpath = os.path.join(dirpath, f)\n",
    "        size = os.path.getsize(fpath)\n",
    "        if size > 1e6:\n",
    "            size_str = f\"{size / 1e6:.1f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{size / 1e3:.0f} KB\"\n",
    "        # Count lines\n",
    "        n_lines = sum(1 for _ in open(fpath))\n",
    "        print(f\"{subindent}{f:<50s} {size_str:>10s}  ({n_lines:,} lines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665fe3b",
   "metadata": {},
   "source": [
    "## 5. Explore the Unified Consensus Peaks\n",
    "\n",
    "The unified BED has columns: `chr, start, end, peak_id, species_detected`. The species_detected column is a comma-separated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load and explore unified consensus peaks\n",
    "# =============================================================================\n",
    "unified_bed = results[\"output_files\"][\"unified_consensus\"]\n",
    "unified_df = pd.read_csv(unified_bed, sep=\"\\t\", header=None,\n",
    "                         names=[\"chr\", \"start\", \"end\", \"peak_id\", \"species_detected\"])\n",
    "\n",
    "print(f\"Unified consensus peaks: {len(unified_df):,}\")\n",
    "print(f\"\\nFirst 10 peaks:\")\n",
    "print(unified_df.head(10).to_string(index=False))\n",
    "\n",
    "# Parse species detection\n",
    "all_species = [\"Bonobo\", \"Chimpanzee\", \"Gorilla\", \"Human\", \"Macaque\", \"Marmoset\"]\n",
    "for sp in all_species:\n",
    "    unified_df[f\"detected_in_{sp}\"] = unified_df[\"species_detected\"].str.contains(sp, na=False)\n",
    "\n",
    "# Count species per peak\n",
    "unified_df[\"n_species\"] = unified_df[[f\"detected_in_{sp}\" for sp in all_species]].sum(axis=1)\n",
    "\n",
    "print(f\"\\nSpecies detection distribution:\")\n",
    "print(unified_df[\"n_species\"].value_counts().sort_index().to_string())\n",
    "\n",
    "# Per-species detection rate\n",
    "print(f\"\\nPer-species detection:\")\n",
    "for sp in all_species:\n",
    "    col = f\"detected_in_{sp}\"\n",
    "    n = unified_df[col].sum()\n",
    "    pct = n / len(unified_df) * 100\n",
    "    print(f\"  {sp:<15s}: {n:>10,} peaks ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b691d",
   "metadata": {},
   "source": [
    "## 6. Explore Human-Specific and Species-Specific Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Human-specific peaks\n",
    "# =============================================================================\n",
    "hs_bed = results[\"output_files\"][\"human_specific\"]\n",
    "hs_df = pd.read_csv(hs_bed, sep=\"\\t\", header=None,\n",
    "                     names=[\"chr\", \"start\", \"end\", \"peak_id\"])\n",
    "\n",
    "print(f\"Human-specific peaks: {len(hs_df):,}\")\n",
    "print(f\"First 5:\")\n",
    "print(hs_df.head().to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# Species-specific peaks\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Species-specific peaks:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "species_specific_counts = {}\n",
    "for species in SPECIES_BEDS:\n",
    "    key = f\"species_specific_{species}\"\n",
    "    if key in results[\"output_files\"]:\n",
    "        sp_bed = results[\"output_files\"][key]\n",
    "        if os.path.exists(sp_bed):\n",
    "            sp_df = pd.read_csv(sp_bed, sep=\"\\t\", header=None,\n",
    "                                names=[\"chr\", \"start\", \"end\", \"peak_id\"])\n",
    "            species_specific_counts[species] = len(sp_df)\n",
    "            print(f\"\\n  {species}: {len(sp_df):,} specific peaks\")\n",
    "            print(f\"  Example IDs: {', '.join(sp_df['peak_id'].head(3))}\")\n",
    "\n",
    "# Summary bar\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Unified consensus:   {len(unified_df):>10,}\")\n",
    "print(f\"  Human-specific:      {len(hs_df):>10,}\")\n",
    "for sp, n in species_specific_counts.items():\n",
    "    print(f\"  {sp}-specific: {n:>10,}\")\n",
    "total = len(unified_df) + len(hs_df) + sum(species_specific_counts.values())\n",
    "print(f\"  {'TOTAL':>22s}: {total:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693704f",
   "metadata": {},
   "source": [
    "## 7. Explore Peak Annotation File\n",
    "\n",
    "The master annotation file contains: `peak_id, chr, start, end, peak_type, species_detected, closest_gene, distance_to_gene`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b82c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load and explore the master peak annotation\n",
    "# =============================================================================\n",
    "annotation_file = results[\"output_files\"][\"annotation\"]\n",
    "annot_df = pd.read_csv(annotation_file, sep=\"\\t\")\n",
    "\n",
    "print(f\"Total annotated peaks: {len(annot_df):,}\")\n",
    "print(f\"\\nColumns: {list(annot_df.columns)}\")\n",
    "print(f\"\\nPeak types:\")\n",
    "print(annot_df[\"peak_type\"].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nDistance to nearest gene (summary):\")\n",
    "valid_dist = annot_df[annot_df[\"distance_to_gene\"] >= 0][\"distance_to_gene\"]\n",
    "print(f\"  Median: {valid_dist.median():,.0f} bp\")\n",
    "print(f\"  Mean:   {valid_dist.mean():,.0f} bp\")\n",
    "print(f\"  At TSS (0 bp): {(valid_dist == 0).sum():,}\")\n",
    "print(f\"  < 1 kb:  {(valid_dist < 1000).sum():,}\")\n",
    "print(f\"  < 10 kb: {(valid_dist < 10000).sum():,}\")\n",
    "print(f\"  < 100 kb: {(valid_dist < 100000).sum():,}\")\n",
    "\n",
    "print(f\"\\nSample rows from each peak type:\")\n",
    "for pt in annot_df[\"peak_type\"].unique():\n",
    "    subset = annot_df[annot_df[\"peak_type\"] == pt].head(3)\n",
    "    print(f\"\\n  {pt}:\")\n",
    "    print(subset[[\"peak_id\", \"chr\", \"start\", \"end\", \"species_detected\", \"closest_gene\", \"distance_to_gene\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdb10a",
   "metadata": {},
   "source": [
    "## 8. Build Combined BED File\n",
    "\n",
    "Write a single BED file with all peak categories: unified (hg38), human-specific (hg38), and species-specific (native coords). Also write per-species BED files with liftback unified + species-specific peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Build combined BED file (all peak categories)\n",
    "# =============================================================================\n",
    "combined_dir = os.path.join(OUTPUT_DIR, \"07_combined\")\n",
    "os.makedirs(combined_dir, exist_ok=True)\n",
    "\n",
    "combined_bed = os.path.join(combined_dir, \"all_peaks_combined.bed\")\n",
    "\n",
    "with open(combined_bed, 'w') as fout:\n",
    "    # Header as a comment\n",
    "    fout.write(\"#chr\\tstart\\tend\\tpeak_id\\tcategory\\tgenome_assembly\\n\")\n",
    "\n",
    "    # 1. Unified peaks (hg38)\n",
    "    n_unified = 0\n",
    "    with open(results[\"output_files\"][\"unified_consensus\"]) as fin:\n",
    "        for line in fin:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\tunified\\thg38\\n\")\n",
    "                n_unified += 1\n",
    "\n",
    "    # 2. Human-specific peaks (hg38)\n",
    "    n_human_spec = 0\n",
    "    with open(results[\"output_files\"][\"human_specific\"]) as fin:\n",
    "        for line in fin:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\thuman_specific\\thg38\\n\")\n",
    "                n_human_spec += 1\n",
    "\n",
    "    # 3. Species-specific peaks (native coordinates)\n",
    "    assembly_map = {\n",
    "        \"Bonobo\": \"panPan2\", \"Chimpanzee\": \"panTro5\", \"Gorilla\": \"gorGor4\",\n",
    "        \"Macaque\": \"rheMac10\", \"Marmoset\": \"calJac1\",\n",
    "    }\n",
    "    n_sp_spec = 0\n",
    "    for species in SPECIES_BEDS:\n",
    "        key = f\"species_specific_{species}\"\n",
    "        if key in results[\"output_files\"] and os.path.exists(results[\"output_files\"][key]):\n",
    "            assembly = assembly_map.get(species, species)\n",
    "            with open(results[\"output_files\"][key]) as fin:\n",
    "                for line in fin:\n",
    "                    if line.strip() and not line.startswith('#'):\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\t{species.lower()}_specific\\t{assembly}\\n\")\n",
    "                        n_sp_spec += 1\n",
    "\n",
    "print(f\"Combined BED written: {combined_bed}\")\n",
    "print(f\"  Unified:          {n_unified:,}\")\n",
    "print(f\"  Human-specific:   {n_human_spec:,}\")\n",
    "print(f\"  Species-specific: {n_sp_spec:,}\")\n",
    "print(f\"  Total:            {n_unified + n_human_spec + n_sp_spec:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Per-species BED files (liftback unified + species-specific, in native coords)\n",
    "# =============================================================================\n",
    "print(f\"\\nPer-species complete peak sets:\")\n",
    "\n",
    "for species in SPECIES_BEDS:\n",
    "    sp_combined = os.path.join(combined_dir, f\"all_peaks_{species}.bed\")\n",
    "    n = 0\n",
    "\n",
    "    with open(sp_combined, 'w') as fout:\n",
    "        # Liftback unified peaks\n",
    "        liftback_key = f\"liftback_{species}\"\n",
    "        if liftback_key in results[\"output_files\"]:\n",
    "            liftback_file = results[\"output_files\"][liftback_key]\n",
    "            if os.path.exists(liftback_file):\n",
    "                with open(liftback_file) as fin:\n",
    "                    for line in fin:\n",
    "                        if line.strip() and not line.startswith('#'):\n",
    "                            parts = line.strip().split('\\t')\n",
    "                            # liftback may have: chr, start, end, peak_id, source_coords\n",
    "                            fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\n\")\n",
    "                            n += 1\n",
    "\n",
    "        # Species-specific peaks\n",
    "        sp_key = f\"species_specific_{species}\"\n",
    "        if sp_key in results[\"output_files\"]:\n",
    "            sp_file = results[\"output_files\"][sp_key]\n",
    "            if os.path.exists(sp_file):\n",
    "                with open(sp_file) as fin:\n",
    "                    for line in fin:\n",
    "                        if line.strip() and not line.startswith('#'):\n",
    "                            parts = line.strip().split('\\t')\n",
    "                            fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\n\")\n",
    "                            n += 1\n",
    "\n",
    "    print(f\"  {species}: {n:,} peaks -> {sp_combined}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7f339",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary statistics and validation\n",
    "# =============================================================================\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Check no duplicate peak IDs\n",
    "all_ids = annot_df[\"peak_id\"].tolist()\n",
    "n_unique = len(set(all_ids))\n",
    "n_total = len(all_ids)\n",
    "print(f\"\\nPeak ID uniqueness: {n_unique:,} unique / {n_total:,} total\", end=\"\")\n",
    "if n_unique == n_total:\n",
    "    print(\" -- OK\")\n",
    "else:\n",
    "    dup_ids = annot_df[annot_df[\"peak_id\"].duplicated(keep=False)][\"peak_id\"].unique()\n",
    "    print(f\" -- WARNING: {n_total - n_unique} duplicates!\")\n",
    "    print(f\"  Duplicate IDs: {list(dup_ids[:10])}\")\n",
    "\n",
    "# 2. Liftover success rates\n",
    "print(f\"\\nLiftover success rates (species -> hg38):\")\n",
    "print(f\"  {'Species':<15s} {'Input':>10s} {'Lifted':>10s} {'Rate':>8s}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "for species, res in results[\"lift_to_human\"].items():\n",
    "    inp = res.get(\"total\", res.get(\"input\", 0))\n",
    "    lifted = res.get(\"lifted\", 0)\n",
    "    rate = lifted / inp * 100 if inp > 0 else 0\n",
    "    print(f\"  {species:<15s} {inp:>10,} {lifted:>10,} {rate:>7.1f}%\")\n",
    "\n",
    "print(f\"\\nLiftback success rates (hg38 -> species):\")\n",
    "print(f\"  {'Species':<15s} {'Input':>10s} {'Lifted':>10s} {'Rate':>8s}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "for species, res in results[\"lift_back\"].items():\n",
    "    inp = res.get(\"total\", res.get(\"input\", 0))\n",
    "    lifted = res.get(\"lifted\", 0)\n",
    "    rate = lifted / inp * 100 if inp > 0 else 0\n",
    "    print(f\"  {species:<15s} {inp:>10,} {lifted:>10,} {rate:>7.1f}%\")\n",
    "\n",
    "# 3. Species detection distribution plot\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SPECIES DETECTION IN UNIFIED PEAKS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: peaks per n_species\n",
    "n_species_counts = unified_df[\"n_species\"].value_counts().sort_index()\n",
    "axes[0].bar(n_species_counts.index, n_species_counts.values, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].set_xlabel(\"Number of species detected\")\n",
    "axes[0].set_ylabel(\"Number of peaks\")\n",
    "axes[0].set_title(\"Distribution of species detection\\n(unified consensus)\")\n",
    "for x, y in zip(n_species_counts.index, n_species_counts.values):\n",
    "    axes[0].text(x, y, f\"{y:,}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "# Bar chart: peak category counts\n",
    "categories = {\"Unified\": len(unified_df), \"Human-specific\": len(hs_df)}\n",
    "for sp, n in species_specific_counts.items():\n",
    "    categories[f\"{sp}-specific\"] = n\n",
    "\n",
    "cats = list(categories.keys())\n",
    "vals = list(categories.values())\n",
    "colors = [\"steelblue\"] + [\"firebrick\"] + [\"seagreen\"] * len(species_specific_counts)\n",
    "axes[1].barh(cats, vals, color=colors, edgecolor=\"white\")\n",
    "axes[1].set_xlabel(\"Number of peaks\")\n",
    "axes[1].set_title(\"Peak counts by category\")\n",
    "for y_pos, v in enumerate(vals):\n",
    "    axes[1].text(v, y_pos, f\"  {v:,}\", ha=\"left\", va=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = os.path.join(OUTPUT_DIR, \"peak_summary.png\")\n",
    "plt.savefig(plot_file, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nSaved summary plot: {plot_file}\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Save summary stats\n",
    "summary_file = os.path.join(OUTPUT_DIR, \"pipeline_summary.txt\")\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"Cross-Species Consensus Pipeline v2 -- Summary\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Unified consensus peaks: {len(unified_df):,}\\n\")\n",
    "    f.write(f\"Human-specific peaks:    {len(hs_df):,}\\n\")\n",
    "    for sp, n in species_specific_counts.items():\n",
    "        f.write(f\"{sp}-specific peaks:  {n:,}\\n\")\n",
    "    f.write(f\"\\nSpecies detection distribution (unified):\\n\")\n",
    "    for n_sp, count in n_species_counts.items():\n",
    "        f.write(f\"  {n_sp} species: {count:,} peaks\\n\")\n",
    "    f.write(f\"\\nAnnotation file: {annotation_file}\\n\")\n",
    "    f.write(f\"Combined BED:    {combined_bed}\\n\")\n",
    "\n",
    "print(f\"Saved summary: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
