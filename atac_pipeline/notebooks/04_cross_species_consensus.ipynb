{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c371fa",
   "metadata": {},
   "source": [
    "# Cross-Species Consensus Peak Pipeline (v2)\n",
    "\n",
    "**Goal:** Build a unified cross-species ATAC-seq consensus peak set across 6 primate species,\n",
    "with full provenance tracking and gene annotation.\n",
    "\n",
    "**Pipeline overview:**\n",
    "1. Lift all non-human species peaks to hg38\n",
    "2. Merge lifted peaks + human peaks into a unified consensus, tracking which species contributed each peak\n",
    "3. Identify **human-specific peaks** (human peaks not overlapping any non-human lifted peak)\n",
    "4. Lift unified consensus back to each species genome\n",
    "5. Identify **species-specific peaks** (original species peaks not covered by any liftback peak)\n",
    "6. Annotate all peaks with closest gene (from species-appropriate GTF) and distance\n",
    "\n",
    "**Outputs:**\n",
    "- `unified_peak_NNNNNN` -- consensus peaks in hg38 with species-detection annotation\n",
    "- `human_peak_NNNNNN` -- human-specific peaks (hg38) not liftable to any species\n",
    "- `{species}_peak_NNNNNN` -- per-species peaks in native coordinates, not liftable to hg38\n",
    "- `peak_annotation.tsv` -- master annotation: peak_id, type, species detected, closest gene, distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1fe7dd",
   "metadata": {},
   "source": [
    "## 1. Load Packages and Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from src.cross_species import (\n",
    "    cross_species_consensus_pipeline,\n",
    "    merge_with_species_tracking,\n",
    "    find_human_specific_peaks,\n",
    "    find_species_specific_peaks,\n",
    "    create_peak_annotation,\n",
    "    extract_gene_bed_from_gtf,\n",
    "    annotate_with_closest_gene,\n",
    "    add_peak_ids,\n",
    "    build_master_annotation,\n",
    "    cross_map_species_specific_peaks,\n",
    "    DEFAULT_GTF_FILES,\n",
    "    REVERSE_CHAIN_FILES,\n",
    "    CROSS_SPECIES_ROUTES,\n",
    ")\n",
    "from src.liftover import DEFAULT_CHAIN_DIR\n",
    "\n",
    "print(\"All imports loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration -- edit paths here\n",
    "# =============================================================================\n",
    "\n",
    "PEAKS_BASE = \"/cluster/project/treutlein/USERS/jjans/analysis/adult_intestine/peaks\"\n",
    "\n",
    "# Human consensus peaks (separate from non-human species)\n",
    "HUMAN_BED = f\"{PEAKS_BASE}/consensus_peak_calling_Human/Consensus_Peaks_Filtered_500.bed\"\n",
    "\n",
    "# Non-human species consensus peaks\n",
    "SPECIES_BEDS = {\n",
    "    \"Bonobo\":      f\"{PEAKS_BASE}/consensus_peak_calling_Bonobo/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Chimpanzee\":  f\"{PEAKS_BASE}/consensus_peak_calling_Chimpanzee/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Gorilla\":     f\"{PEAKS_BASE}/consensus_peak_calling_Gorilla/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Macaque\":     f\"{PEAKS_BASE}/consensus_peak_calling_Macaque/Consensus_Peaks_Filtered_500.bed\",\n",
    "    \"Marmoset\":    f\"{PEAKS_BASE}/consensus_peak_calling_Marmoset/Consensus_Peaks_Filtered_500.bed\",\n",
    "}\n",
    "\n",
    "# Pre-lifted BED files (already in hg38 coords from previous liftover run)\n",
    "# Set to None to re-run liftover from scratch\n",
    "PRE_LIFTED_BEDS = {\n",
    "    \"Bonobo\":      f\"{PEAKS_BASE}/lifted_consensus_peaks/Consensus_Peaks_Filtered_500.hg38_Bonobo.bed\",\n",
    "    \"Chimpanzee\":  f\"{PEAKS_BASE}/lifted_consensus_peaks/Consensus_Peaks_Filtered_500.hg38_Chimpanzee.bed\",\n",
    "    \"Gorilla\":     f\"{PEAKS_BASE}/lifted_consensus_peaks/Consensus_Peaks_Filtered_500.hg38_Gorilla.bed\",\n",
    "    \"Macaque\":     f\"{PEAKS_BASE}/lifted_consensus_peaks/Consensus_Peaks_Filtered_500.hg38_Macaque.bed\",\n",
    "    \"Marmoset\":    f\"{PEAKS_BASE}/lifted_consensus_peaks/Consensus_Peaks_Filtered_500.hg38_Marmoset.bed\",\n",
    "}\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f\"{PEAKS_BASE}/cross_species_consensus_v2\"\n",
    "\n",
    "# liftOver executable\n",
    "LIFTOVER_PATH = \"/cluster/project/treutlein/jjans/software/miniforge3/envs/genomes/bin/liftOver\"\n",
    "\n",
    "# Chain file directory\n",
    "CHAIN_DIR = DEFAULT_CHAIN_DIR\n",
    "\n",
    "# GTF files for gene annotation (use defaults from cross_species.py)\n",
    "GTF_FILES = DEFAULT_GTF_FILES.copy()\n",
    "\n",
    "# Per-species minimum match ratios for liftOver\n",
    "# Great apes are closer to human -> higher match; more distant species -> lower\n",
    "MIN_MATCH = {\n",
    "    \"Bonobo\":      0.9,\n",
    "    \"Chimpanzee\":  0.9,\n",
    "    \"Gorilla\":     0.9,\n",
    "    \"Macaque\":     0.8,\n",
    "    \"Marmoset\":    0.6,\n",
    "}\n",
    "\n",
    "MERGE_DISTANCE = 0  # Max distance between peaks to merge (0 = must overlap)\n",
    "NCPU = 16           # Parallel workers for liftover + cross-mapping\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Chain file dir:   {CHAIN_DIR}\")\n",
    "print(f\"liftOver binary:  {LIFTOVER_PATH}\")\n",
    "print(f\"Species:          {list(SPECIES_BEDS.keys())}\")\n",
    "print(f\"Min match rates:  {MIN_MATCH}\")\n",
    "print(f\"Pre-lifted beds:  {'Yes (skipping step 1)' if PRE_LIFTED_BEDS else 'No (will run liftover)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fa36f",
   "metadata": {},
   "source": [
    "## 2. Validate Input Files\n",
    "\n",
    "Check that all input peak files, chain files, and GTF files exist before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e676c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Validate all input files exist\n",
    "# =============================================================================\n",
    "from src.liftover import CHAIN_FILES, get_chain_file\n",
    "\n",
    "all_ok = True\n",
    "\n",
    "# Check human BED\n",
    "print(\"--- Human peaks ---\")\n",
    "if os.path.exists(HUMAN_BED):\n",
    "    n = sum(1 for l in open(HUMAN_BED) if l.strip() and not l.startswith('#'))\n",
    "    print(f\"  OK  Human: {n:,} peaks\")\n",
    "else:\n",
    "    print(f\"  MISSING  {HUMAN_BED}\")\n",
    "    all_ok = False\n",
    "\n",
    "# Check non-human BED files\n",
    "print(\"\\n--- Non-human species peaks ---\")\n",
    "for species, bed in SPECIES_BEDS.items():\n",
    "    if os.path.exists(bed):\n",
    "        n = sum(1 for l in open(bed) if l.strip() and not l.startswith('#'))\n",
    "        print(f\"  OK  {species}: {n:,} peaks\")\n",
    "    else:\n",
    "        print(f\"  MISSING  {species}: {bed}\")\n",
    "        all_ok = False\n",
    "\n",
    "# Check pre-lifted BED files (if provided)\n",
    "if PRE_LIFTED_BEDS:\n",
    "    print(\"\\n--- Pre-lifted BED files (hg38 coords) ---\")\n",
    "    for species, bed in PRE_LIFTED_BEDS.items():\n",
    "        if os.path.exists(bed):\n",
    "            n = sum(1 for l in open(bed) if l.strip() and not l.startswith('#'))\n",
    "            print(f\"  OK  {species}: {n:,} peaks\")\n",
    "        else:\n",
    "            print(f\"  MISSING  {species}: {bed}\")\n",
    "            all_ok = False\n",
    "\n",
    "# Check chain files (forward: species -> hg38)\n",
    "# Only needed if we're actually running liftover (no pre-lifted beds)\n",
    "needs_liftover = not PRE_LIFTED_BEDS or any(\n",
    "    sp not in PRE_LIFTED_BEDS or not os.path.exists(PRE_LIFTED_BEDS[sp])\n",
    "    for sp in SPECIES_BEDS\n",
    ")\n",
    "\n",
    "if needs_liftover:\n",
    "    print(\"\\n--- Forward chain files (species -> hg38) ---\")\n",
    "    for species in SPECIES_BEDS:\n",
    "        if species == \"Marmoset\":\n",
    "            for step in [\"Marmoset_step1\", \"Marmoset_step2\"]:\n",
    "                path = os.path.join(CHAIN_DIR, CHAIN_FILES[step])\n",
    "                status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "                print(f\"  {status}  {step}: {CHAIN_FILES[step]}\")\n",
    "                if status == \"MISSING\":\n",
    "                    all_ok = False\n",
    "        else:\n",
    "            path = os.path.join(CHAIN_DIR, CHAIN_FILES[species])\n",
    "            status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "            print(f\"  {status}  {species}: {CHAIN_FILES[species]}\")\n",
    "            if status == \"MISSING\":\n",
    "                all_ok = False\n",
    "else:\n",
    "    print(\"\\n--- Forward chain files --- SKIPPED (using pre-lifted files)\")\n",
    "\n",
    "# Check reverse chain files (hg38 -> species, always needed for liftback)\n",
    "print(\"\\n--- Reverse chain files (hg38 -> species) ---\")\n",
    "for key, chain in REVERSE_CHAIN_FILES.items():\n",
    "    path = os.path.join(CHAIN_DIR, chain)\n",
    "    status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "    print(f\"  {status}  {key}: {chain}\")\n",
    "    if status == \"MISSING\":\n",
    "        all_ok = False\n",
    "\n",
    "# Check GTF files\n",
    "print(\"\\n--- GTF files for gene annotation ---\")\n",
    "for species, gtf in GTF_FILES.items():\n",
    "    status = \"OK\" if os.path.exists(gtf) else \"MISSING\"\n",
    "    print(f\"  {status}  {species}: {os.path.basename(gtf)}\")\n",
    "    if status == \"MISSING\":\n",
    "        all_ok = False\n",
    "\n",
    "# Check liftOver binary\n",
    "print(f\"\\n--- liftOver binary ---\")\n",
    "status = \"OK\" if os.path.exists(LIFTOVER_PATH) else \"MISSING\"\n",
    "print(f\"  {status}  {LIFTOVER_PATH}\")\n",
    "if status == \"MISSING\":\n",
    "    all_ok = False\n",
    "\n",
    "print(f\"\\n{'All inputs validated successfully' if all_ok else 'WARNING: Some inputs are missing!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1307f",
   "metadata": {},
   "source": [
    "## 3. Run Cross-Species Consensus Pipeline\n",
    "\n",
    "This executes all 6 steps:\n",
    "1. **Lift to hg38** -- liftOver each non-human species to human genome (skipped if `PRE_LIFTED_BEDS` are provided)\n",
    "2. **Merge with tracking** -- merge all species (incl. human) with bedtools, recording which species contributed\n",
    "3. **Human-specific** -- human peaks not overlapping any lifted non-human peak\n",
    "4. **Lift back** -- lift unified consensus back to each species' genome (uses per-species `MIN_MATCH`)\n",
    "5. **Species-specific** -- original species peaks not covered by any liftback peak\n",
    "6. **Annotate** -- closest gene and distance for every peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503dbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Run the full pipeline\n",
    "# =============================================================================\n",
    "results = cross_species_consensus_pipeline(\n",
    "    species_beds=SPECIES_BEDS,\n",
    "    human_bed=HUMAN_BED,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    chain_dir=CHAIN_DIR,\n",
    "    liftover_path=LIFTOVER_PATH,\n",
    "    min_match=MIN_MATCH,\n",
    "    merge_distance=MERGE_DISTANCE,\n",
    "    peak_prefix=\"unified\",\n",
    "    gtf_files=GTF_FILES,\n",
    "    pre_lifted_beds=PRE_LIFTED_BEDS,\n",
    "    verbose=True,\n",
    "    ncpu=NCPU,\n",
    ")\n",
    "\n",
    "print(f\"\\nPipeline status: {results['status']}\")\n",
    "print(f\"Message: {results['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03fda3c",
   "metadata": {},
   "source": [
    "## 4. Inspect Output Files\n",
    "\n",
    "Check what was produced in each output subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# List all output files with sizes\n",
    "# =============================================================================\n",
    "import subprocess\n",
    "\n",
    "print(\"Output directory structure:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for dirpath, dirnames, filenames in sorted(os.walk(OUTPUT_DIR)):\n",
    "    level = dirpath.replace(OUTPUT_DIR, \"\").count(os.sep)\n",
    "    indent = \"  \" * level\n",
    "    reldir = os.path.relpath(dirpath, OUTPUT_DIR)\n",
    "    print(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "    subindent = \"  \" * (level + 1)\n",
    "    for f in sorted(filenames):\n",
    "        fpath = os.path.join(dirpath, f)\n",
    "        size = os.path.getsize(fpath)\n",
    "        if size > 1e6:\n",
    "            size_str = f\"{size / 1e6:.1f} MB\"\n",
    "        else:\n",
    "            size_str = f\"{size / 1e3:.0f} KB\"\n",
    "        # Count lines (skip binary files)\n",
    "        try:\n",
    "            n_lines = sum(1 for _ in open(fpath, errors=\"replace\"))\n",
    "            print(f\"{subindent}{f:<50s} {size_str:>10s}  ({n_lines:,} lines)\")\n",
    "        except Exception:\n",
    "            print(f\"{subindent}{f:<50s} {size_str:>10s}  (binary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665fe3b",
   "metadata": {},
   "source": [
    "## 5. Explore the Unified Consensus Peaks\n",
    "\n",
    "The unified BED has columns: `chr, start, end, peak_id, species_detected`. The species_detected column is a comma-separated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load and explore unified consensus peaks\n",
    "# =============================================================================\n",
    "unified_bed = results[\"output_files\"][\"unified_consensus\"]\n",
    "unified_df = pd.read_csv(unified_bed, sep=\"\\t\", header=None,\n",
    "                         names=[\"chr\", \"start\", \"end\", \"peak_id\", \"species_detected\"])\n",
    "\n",
    "print(f\"Unified consensus peaks: {len(unified_df):,}\")\n",
    "print(f\"\\nFirst 10 peaks:\")\n",
    "print(unified_df.head(10).to_string(index=False))\n",
    "\n",
    "# Parse species detection\n",
    "all_species = [\"Bonobo\", \"Chimpanzee\", \"Gorilla\", \"Human\", \"Macaque\", \"Marmoset\"]\n",
    "for sp in all_species:\n",
    "    unified_df[f\"detected_in_{sp}\"] = unified_df[\"species_detected\"].str.contains(sp, na=False)\n",
    "\n",
    "# Count species per peak\n",
    "unified_df[\"n_species\"] = unified_df[[f\"detected_in_{sp}\" for sp in all_species]].sum(axis=1)\n",
    "\n",
    "print(f\"\\nSpecies detection distribution:\")\n",
    "print(unified_df[\"n_species\"].value_counts().sort_index().to_string())\n",
    "\n",
    "# Per-species detection rate\n",
    "print(f\"\\nPer-species detection:\")\n",
    "for sp in all_species:\n",
    "    col = f\"detected_in_{sp}\"\n",
    "    n = unified_df[col].sum()\n",
    "    pct = n / len(unified_df) * 100\n",
    "    print(f\"  {sp:<15s}: {n:>10,} peaks ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1292c50",
   "metadata": {},
   "source": [
    "## 5b. UpSet Plot â€” Species Detection Overlap\n",
    "\n",
    "UpSet plot showing which species combinations share peaks.\n",
    "Each column in the intersection matrix represents a unique combination\n",
    "of species in which a peak was detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UpSet plot: species detection overlap\n",
    "# =============================================================================\n",
    "from src.visualization import plot_upset\n",
    "\n",
    "all_species = [\"Human\", \"Bonobo\", \"Chimpanzee\", \"Gorilla\", \"Macaque\", \"Marmoset\"]\n",
    "species_cols = [f\"detected_in_{sp}\" for sp in all_species]\n",
    "\n",
    "plot_file = os.path.join(OUTPUT_DIR, \"upset_species_detection.png\")\n",
    "\n",
    "fig = plot_upset(\n",
    "    unified_df,\n",
    "    set_columns=species_cols,\n",
    "    set_labels=all_species,\n",
    "    top_n=30,\n",
    "    color=\"steelblue\",\n",
    "    title=\"Peak Detection Across Primate Species\",\n",
    "    saveas=plot_file,\n",
    ")\n",
    "\n",
    "# --- print top intersections ---\n",
    "upset_df = unified_df[species_cols].copy()\n",
    "upset_df.columns = all_species\n",
    "pattern = upset_df.astype(int).apply(tuple, axis=1)\n",
    "combo_counts = pattern.value_counts().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nTop 15 intersections:\")\n",
    "for combo_tuple, count in combo_counts.head(15).items():\n",
    "    names = [sp for sp, flag in zip(all_species, combo_tuple) if flag]\n",
    "    pct = count / len(unified_df) * 100\n",
    "    print(f\"  {', '.join(names):<55s} {count:>8,} peaks ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b691d",
   "metadata": {},
   "source": [
    "## 6. Explore Human-Specific and Species-Specific Peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Human-specific peaks\n",
    "# =============================================================================\n",
    "hs_bed = results[\"output_files\"][\"human_specific\"]\n",
    "hs_df = pd.read_csv(hs_bed, sep=\"\\t\", header=None,\n",
    "                     names=[\"chr\", \"start\", \"end\", \"peak_id\"])\n",
    "\n",
    "print(f\"Human-specific peaks: {len(hs_df):,}\")\n",
    "print(f\"First 5:\")\n",
    "print(hs_df.head().to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# Species-specific peaks\n",
    "# =============================================================================\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Species-specific peaks:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "species_specific_counts = {}\n",
    "for species in SPECIES_BEDS:\n",
    "    key = f\"species_specific_{species}\"\n",
    "    if key in results[\"output_files\"]:\n",
    "        sp_bed = results[\"output_files\"][key]\n",
    "        if os.path.exists(sp_bed):\n",
    "            sp_df = pd.read_csv(sp_bed, sep=\"\\t\", header=None,\n",
    "                                names=[\"chr\", \"start\", \"end\", \"peak_id\"])\n",
    "            species_specific_counts[species] = len(sp_df)\n",
    "            print(f\"\\n  {species}: {len(sp_df):,} specific peaks\")\n",
    "            print(f\"  Example IDs: {', '.join(sp_df['peak_id'].head(3))}\")\n",
    "\n",
    "# Summary bar\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Unified consensus:   {len(unified_df):>10,}\")\n",
    "print(f\"  Human-specific:      {len(hs_df):>10,}\")\n",
    "for sp, n in species_specific_counts.items():\n",
    "    print(f\"  {sp}-specific: {n:>10,}\")\n",
    "total = len(unified_df) + len(hs_df) + sum(species_specific_counts.values())\n",
    "print(f\"  {'TOTAL':>22s}: {total:>10,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9693704f",
   "metadata": {},
   "source": [
    "## 7. Explore Peak Annotation File\n",
    "\n",
    "The master annotation file contains: `peak_id, chr, start, end, peak_type, species_detected, closest_gene, distance_to_gene`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b82c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load and explore the master peak annotation\n",
    "# =============================================================================\n",
    "annotation_file = results[\"output_files\"][\"annotation\"]\n",
    "annot_df = pd.read_csv(annotation_file, sep=\"\\t\")\n",
    "\n",
    "print(f\"Total annotated peaks: {len(annot_df):,}\")\n",
    "print(f\"\\nColumns: {list(annot_df.columns)}\")\n",
    "print(f\"\\nPeak types:\")\n",
    "print(annot_df[\"peak_type\"].value_counts().to_string())\n",
    "\n",
    "print(f\"\\nDistance to nearest gene (summary):\")\n",
    "valid_dist = annot_df[annot_df[\"distance_to_gene\"] >= 0][\"distance_to_gene\"]\n",
    "print(f\"  Median: {valid_dist.median():,.0f} bp\")\n",
    "print(f\"  Mean:   {valid_dist.mean():,.0f} bp\")\n",
    "print(f\"  At TSS (0 bp): {(valid_dist == 0).sum():,}\")\n",
    "print(f\"  < 1 kb:  {(valid_dist < 1000).sum():,}\")\n",
    "print(f\"  < 10 kb: {(valid_dist < 10000).sum():,}\")\n",
    "print(f\"  < 100 kb: {(valid_dist < 100000).sum():,}\")\n",
    "\n",
    "print(f\"\\nSample rows from each peak type:\")\n",
    "for pt in annot_df[\"peak_type\"].unique():\n",
    "    subset = annot_df[annot_df[\"peak_type\"] == pt].head(3)\n",
    "    print(f\"\\n  {pt}:\")\n",
    "    print(subset[[\"peak_id\", \"chr\", \"start\", \"end\", \"species_detected\", \"closest_gene\", \"distance_to_gene\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898ee1c",
   "metadata": {},
   "source": [
    "## 7b. Gene Conservation After Liftback\n",
    "\n",
    "For each unified peak we know the closest human gene (hg38 annotation).\n",
    "After lifting those same peaks back to each species' genome, we can ask:\n",
    "**is the closest gene in the species genome the same gene?**\n",
    "\n",
    "This checks whether the syntenic neighbourhood is preserved across the liftover round-trip.\n",
    "Gene name matching is case-insensitive and uses the gene symbol (not Ensembl ID),\n",
    "so conservation rates are a lower bound (orthologs with different names will be counted as mismatches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74946007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Gene conservation analysis: human annotation vs species liftback annotation\n",
    "# =============================================================================\n",
    "import subprocess, tempfile\n",
    "\n",
    "# Load human gene annotation for unified peaks\n",
    "human_annot = pd.read_csv(\n",
    "    os.path.join(OUTPUT_DIR, \"06_annotation\", \"unified_gene_annotation.tsv\"),\n",
    "    sep=\"\\t\",\n",
    ")\n",
    "print(f\"Human annotation: {len(human_annot):,} unified peaks with closest gene\")\n",
    "n_human_ens = human_annot[\"closest_gene\"].str.startswith(\"ENS\").sum()\n",
    "n_human_named = len(human_annot) - n_human_ens\n",
    "print(f\"  Human gene symbols: {n_human_named:,} ({n_human_named/len(human_annot)*100:.0f}%)\")\n",
    "print(f\"  Human Ensembl IDs:  {n_human_ens:,} ({n_human_ens/len(human_annot)*100:.0f}%)\")\n",
    "\n",
    "# Gene BED directory (already extracted by pipeline)\n",
    "gene_bed_dir = os.path.join(OUTPUT_DIR, \"06_annotation\", \"gene_beds\")\n",
    "liftback_dir = os.path.join(OUTPUT_DIR, \"04_lifted_back\")\n",
    "\n",
    "conservation_results = {}\n",
    "\n",
    "for species in SPECIES_BEDS:\n",
    "    liftback_file = os.path.join(liftback_dir, f\"unified_consensus_{species}.bed\")\n",
    "    gene_bed = os.path.join(gene_bed_dir, f\"{species}_genes.bed\")\n",
    "\n",
    "    if not os.path.exists(liftback_file) or not os.path.exists(gene_bed):\n",
    "        print(f\"  Skipping {species} - missing files\")\n",
    "        continue\n",
    "\n",
    "    # Count genes in species gene BED\n",
    "    sp_n_genes = sum(1 for _ in open(gene_bed))\n",
    "\n",
    "    # Run bedtools closest on liftback peaks vs species gene BED\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # Detect chr prefix mismatch and harmonize gene BED\n",
    "        with open(liftback_file) as f:\n",
    "            lb_chrom = f.readline().split('\\t')[0]\n",
    "        with open(gene_bed) as f:\n",
    "            gene_chrom = f.readline().split('\\t')[0]\n",
    "\n",
    "        lb_has_chr = lb_chrom.startswith(\"chr\")\n",
    "        gene_has_chr = gene_chrom.startswith(\"chr\")\n",
    "\n",
    "        gene_bed_use = gene_bed\n",
    "        if lb_has_chr and not gene_has_chr:\n",
    "            gene_bed_use = os.path.join(tmpdir, \"genes_chr.bed\")\n",
    "            with open(gene_bed) as fin, open(gene_bed_use, 'w') as fout:\n",
    "                for line in fin:\n",
    "                    if line.strip():\n",
    "                        fout.write(\"chr\" + line)\n",
    "        elif not lb_has_chr and gene_has_chr:\n",
    "            gene_bed_use = os.path.join(tmpdir, \"genes_nochr.bed\")\n",
    "            with open(gene_bed) as fin, open(gene_bed_use, 'w') as fout:\n",
    "                for line in fin:\n",
    "                    if line.strip():\n",
    "                        fout.write(line[3:] if line.startswith(\"chr\") else line)\n",
    "\n",
    "        # Sort both files\n",
    "        lb_sorted = os.path.join(tmpdir, \"lb_sorted.bed\")\n",
    "        gene_sorted = os.path.join(tmpdir, \"gene_sorted.bed\")\n",
    "        subprocess.run(f\"sort -k1,1 -k2,2n {liftback_file} > {lb_sorted}\",\n",
    "                       shell=True, check=True)\n",
    "        subprocess.run(f\"sort -k1,1 -k2,2n {gene_bed_use} > {gene_sorted}\",\n",
    "                       shell=True, check=True)\n",
    "\n",
    "        # bedtools closest\n",
    "        closest_out = os.path.join(tmpdir, \"closest.tsv\")\n",
    "        subprocess.run(\n",
    "            f\"bedtools closest -a {lb_sorted} -b {gene_sorted} -d -t first > {closest_out}\",\n",
    "            shell=True, check=True,\n",
    "        )\n",
    "\n",
    "        # Detect liftback column count\n",
    "        with open(lb_sorted) as f:\n",
    "            n_lb_cols = len(f.readline().strip().split('\\t'))\n",
    "\n",
    "        rows = []\n",
    "        with open(closest_out) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < n_lb_cols + 7:\n",
    "                    continue\n",
    "                peak_id = parts[3]\n",
    "                sp_gene = parts[n_lb_cols + 3]\n",
    "                sp_dist = int(parts[-1]) if parts[-1] != '.' else -1\n",
    "                rows.append({\"peak_id\": peak_id, \"sp_gene\": sp_gene, \"sp_distance\": sp_dist})\n",
    "\n",
    "    sp_annot = pd.DataFrame(rows)\n",
    "\n",
    "    # Merge with human annotation\n",
    "    merged = human_annot.merge(sp_annot, on=\"peak_id\", how=\"inner\")\n",
    "    merged = merged[merged[\"sp_distance\"] >= 0]  # drop unmatched\n",
    "\n",
    "    # Flag Ensembl IDs\n",
    "    merged[\"human_is_ens\"] = merged[\"closest_gene\"].str.startswith(\"ENS\")\n",
    "    merged[\"sp_is_ens\"] = merged[\"sp_gene\"].str.startswith(\"ENS\")\n",
    "\n",
    "    # Compare gene names (case-insensitive)\n",
    "    merged[\"same_gene\"] = merged[\"closest_gene\"].str.upper() == merged[\"sp_gene\"].str.upper()\n",
    "\n",
    "    # Categories\n",
    "    both_named = ~merged[\"human_is_ens\"] & ~merged[\"sp_is_ens\"]\n",
    "    both_ens = merged[\"human_is_ens\"] & merged[\"sp_is_ens\"]\n",
    "    mixed = (merged[\"human_is_ens\"] & ~merged[\"sp_is_ens\"]) | (~merged[\"human_is_ens\"] & merged[\"sp_is_ens\"])\n",
    "\n",
    "    # Restrict to peaks where BOTH annotations are near a gene (<10kb)\n",
    "    near_gene = (merged[\"distance_to_gene\"] < 10000) & (merged[\"sp_distance\"] < 10000)\n",
    "    near_both_named = both_named & near_gene\n",
    "\n",
    "    n_total = len(merged)\n",
    "    n_both_named = both_named.sum()\n",
    "    n_same_named = merged.loc[both_named, \"same_gene\"].sum()\n",
    "    n_near_both_named = near_both_named.sum()\n",
    "    n_same_near = merged.loc[near_both_named, \"same_gene\"].sum()\n",
    "\n",
    "    conservation_results[species] = {\n",
    "        \"total_peaks\": n_total,\n",
    "        \"sp_n_genes\": sp_n_genes,\n",
    "        \"both_named\": n_both_named,\n",
    "        \"same_named\": n_same_named,\n",
    "        \"pct_named\": n_same_named / n_both_named * 100 if n_both_named else 0,\n",
    "        \"near_both_named\": n_near_both_named,\n",
    "        \"same_near\": n_same_near,\n",
    "        \"pct_near\": n_same_near / n_near_both_named * 100 if n_near_both_named else 0,\n",
    "        \"both_ens\": both_ens.sum(),\n",
    "        \"mixed\": mixed.sum(),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{species} ({sp_n_genes:,} gene TSSs in GTF):\")\n",
    "    print(f\"  Peaks compared:                      {n_total:>10,}\")\n",
    "    print(f\"  Both have gene symbol:               {n_both_named:>10,}  -> same gene: {n_same_named:,} ({n_same_named/n_both_named*100:.1f}%)\" if n_both_named else \"\")\n",
    "    print(f\"  Both named + both <10kb from gene:   {n_near_both_named:>10,}  -> same gene: {n_same_near:,} ({n_same_near/n_near_both_named*100:.1f}%)\" if n_near_both_named else \"\")\n",
    "    print(f\"  Both Ensembl IDs (can't compare):    {both_ens.sum():>10,}\")\n",
    "    print(f\"  Mixed (Ensembl vs symbol):            {mixed.sum():>10,}\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"GENE CONSERVATION SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  {'Species':<15s} {'Genes':>7s} {'Both named':>12s} {'Same':>8s} {'%':>6s}  {'Near(<10kb)':>12s} {'Same':>8s} {'%':>6s}\")\n",
    "print(f\"  {'-'*78}\")\n",
    "for species, r in conservation_results.items():\n",
    "    print(f\"  {species:<15s} {r['sp_n_genes']:>7,} {r['both_named']:>12,} {r['same_named']:>8,} {r['pct_named']:>5.1f}%  {r['near_both_named']:>12,} {r['same_near']:>8,} {r['pct_near']:>5.1f}%\")\n",
    "\n",
    "print(f\"\\n  Note: 'Both named' = both human and species annotation are gene symbols (not Ensembl IDs).\")\n",
    "print(f\"  Note: 'Near(<10kb)' = further restricted to peaks <10kb from gene on BOTH sides.\")\n",
    "print(f\"  Peaks far from any gene in the species genome often get a different 'closest gene'\")\n",
    "print(f\"  simply because the species GTF has fewer annotated genes, not because synteny changed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f74b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Detailed breakdown: mismatched genes -- what happened?\n",
    "# =============================================================================\n",
    "\n",
    "# Re-run the merge for one species to inspect mismatches in detail\n",
    "example_species = \"Marmoset\"  # most divergent, interesting to inspect\n",
    "\n",
    "liftback_file = os.path.join(liftback_dir, f\"unified_consensus_{example_species}.bed\")\n",
    "gene_bed = os.path.join(gene_bed_dir, f\"{example_species}_genes.bed\")\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    with open(liftback_file) as f:\n",
    "        lb_chrom = f.readline().split('\\t')[0]\n",
    "    with open(gene_bed) as f:\n",
    "        gene_chrom = f.readline().split('\\t')[0]\n",
    "\n",
    "    lb_has_chr = lb_chrom.startswith(\"chr\")\n",
    "    gene_has_chr = gene_chrom.startswith(\"chr\")\n",
    "    gene_bed_use = gene_bed\n",
    "\n",
    "    if lb_has_chr and not gene_has_chr:\n",
    "        gene_bed_use = os.path.join(tmpdir, \"genes_chr.bed\")\n",
    "        with open(gene_bed) as fin, open(gene_bed_use, 'w') as fout:\n",
    "            for line in fin:\n",
    "                if line.strip():\n",
    "                    fout.write(\"chr\" + line)\n",
    "    elif not lb_has_chr and gene_has_chr:\n",
    "        gene_bed_use = os.path.join(tmpdir, \"genes_nochr.bed\")\n",
    "        with open(gene_bed) as fin, open(gene_bed_use, 'w') as fout:\n",
    "            for line in fin:\n",
    "                if line.strip():\n",
    "                    fout.write(line[3:] if line.startswith(\"chr\") else line)\n",
    "\n",
    "    lb_sorted = os.path.join(tmpdir, \"lb_sorted.bed\")\n",
    "    gene_sorted = os.path.join(tmpdir, \"gene_sorted.bed\")\n",
    "    subprocess.run(f\"sort -k1,1 -k2,2n {liftback_file} > {lb_sorted}\", shell=True, check=True)\n",
    "    subprocess.run(f\"sort -k1,1 -k2,2n {gene_bed_use} > {gene_sorted}\", shell=True, check=True)\n",
    "\n",
    "    closest_out = os.path.join(tmpdir, \"closest.tsv\")\n",
    "    subprocess.run(\n",
    "        f\"bedtools closest -a {lb_sorted} -b {gene_sorted} -d -t first > {closest_out}\",\n",
    "        shell=True, check=True,\n",
    "    )\n",
    "\n",
    "    with open(lb_sorted) as f:\n",
    "        n_lb_cols = len(f.readline().strip().split('\\t'))\n",
    "\n",
    "    rows = []\n",
    "    with open(closest_out) as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < n_lb_cols + 7:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"peak_id\": parts[3],\n",
    "                \"sp_gene\": parts[n_lb_cols + 3],\n",
    "                \"sp_distance\": int(parts[-1]) if parts[-1] != '.' else -1,\n",
    "            })\n",
    "\n",
    "sp_annot = pd.DataFrame(rows)\n",
    "detail = human_annot.merge(sp_annot, on=\"peak_id\", how=\"inner\")\n",
    "detail = detail[detail[\"sp_distance\"] >= 0]\n",
    "detail[\"same_gene\"] = detail[\"closest_gene\"].str.upper() == detail[\"sp_gene\"].str.upper()\n",
    "\n",
    "# Categories\n",
    "detail[\"human_is_ens\"] = detail[\"closest_gene\"].str.startswith(\"ENS\")\n",
    "detail[\"sp_is_ens\"] = detail[\"sp_gene\"].str.startswith(\"ENS\")\n",
    "both_named = ~detail[\"human_is_ens\"] & ~detail[\"sp_is_ens\"]\n",
    "\n",
    "mismatch = detail[both_named & ~detail[\"same_gene\"]]\n",
    "\n",
    "print(f\"=== Gene mismatch breakdown for {example_species} (gene-symbol peaks only) ===\")\n",
    "print(f\"Both have gene symbol: {both_named.sum():,}\")\n",
    "print(f\"Same gene:             {detail.loc[both_named, 'same_gene'].sum():,} ({detail.loc[both_named, 'same_gene'].mean()*100:.1f}%)\")\n",
    "print(f\"Different gene:        {len(mismatch):,}\")\n",
    "\n",
    "# How many mismatches are due to large distance?\n",
    "near_h = mismatch[\"distance_to_gene\"] < 10000\n",
    "near_sp = mismatch[\"sp_distance\"] < 10000\n",
    "print(f\"\\n  Of {len(mismatch):,} mismatches (both gene symbols, different name):\")\n",
    "print(f\"    Both <10kb from gene:  {(near_h & near_sp).sum():>8,}  (true neighbourhood change or naming diff)\")\n",
    "print(f\"    Human >10kb:           {(~near_h).sum():>8,}  (intergenic in human)\")\n",
    "print(f\"    Species >10kb:         {(~near_sp).sum():>8,}  (intergenic in species, sparse GTF?)\")\n",
    "\n",
    "# Show examples of true nearby mismatches\n",
    "true_mm = mismatch[near_h & near_sp]\n",
    "if len(true_mm) > 0:\n",
    "    print(f\"\\n  Examples of nearby mismatches (both <10kb):\")\n",
    "    sample = true_mm.sample(min(10, len(true_mm)), random_state=42)\n",
    "    print(sample[[\"peak_id\", \"closest_gene\", \"distance_to_gene\", \"sp_gene\", \"sp_distance\"]].to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# Visualization\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Bar chart: conservation rate by species (named genes, <10kb)\n",
    "species_names = list(conservation_results.keys())\n",
    "pct_named = [conservation_results[s][\"pct_named\"] for s in species_names]\n",
    "pct_near = [conservation_results[s][\"pct_near\"] for s in species_names]\n",
    "\n",
    "x = np.arange(len(species_names))\n",
    "w = 0.35\n",
    "axes[0].bar(x - w/2, pct_named, w, label=\"Both named (all dist.)\", color=\"steelblue\")\n",
    "axes[0].bar(x + w/2, pct_near, w, label=\"Both named + both <10kb\", color=\"coral\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(species_names, rotation=30, ha=\"right\")\n",
    "axes[0].set_ylabel(\"% same closest gene\")\n",
    "axes[0].set_title(\"Gene conservation after liftback\\n(gene-symbol peaks only)\")\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].set_ylim(0, 100)\n",
    "\n",
    "# 2. Human distance vs species distance scatter for example species\n",
    "# Restrict to both-named\n",
    "detail_named = detail[both_named].copy()\n",
    "sample_scatter = detail_named.sample(min(5000, len(detail_named)), random_state=42)\n",
    "colors = [\"steelblue\" if s else \"firebrick\" for s in sample_scatter[\"same_gene\"]]\n",
    "axes[1].scatter(\n",
    "    np.log10(sample_scatter[\"distance_to_gene\"].clip(lower=1)),\n",
    "    np.log10(sample_scatter[\"sp_distance\"].clip(lower=1)),\n",
    "    c=colors, alpha=0.15, s=3, rasterized=True,\n",
    ")\n",
    "axes[1].plot([0, 6], [0, 6], 'k--', alpha=0.5, lw=0.8)\n",
    "axes[1].set_xlabel(\"log10(human distance to gene)\")\n",
    "axes[1].set_ylabel(f\"log10({example_species} distance to gene)\")\n",
    "axes[1].set_title(f\"TSS distance: human vs {example_species}\\n(blue=same gene, red=different, symbol only)\")\n",
    "axes[1].set_xlim(-0.5, 6.5)\n",
    "axes[1].set_ylim(-0.5, 6.5)\n",
    "\n",
    "# 3. Conservation rate by distance bin (both named)\n",
    "bins = [0, 100, 1000, 5000, 10000, 50000, 100000, 500000, np.inf]\n",
    "labels = [\"0-100\", \"100-1k\", \"1k-5k\", \"5k-10k\", \"10k-50k\", \"50k-100k\", \"100k-500k\", \">500k\"]\n",
    "detail_named[\"dist_bin\"] = pd.cut(detail_named[\"distance_to_gene\"].clip(lower=0), bins=bins, labels=labels)\n",
    "bin_stats = detail_named.groupby(\"dist_bin\", observed=True)[\"same_gene\"].agg([\"mean\", \"count\"])\n",
    "axes[2].bar(range(len(bin_stats)), bin_stats[\"mean\"] * 100, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[2].set_xticks(range(len(bin_stats)))\n",
    "axes[2].set_xticklabels(bin_stats.index, rotation=45, ha=\"right\", fontsize=8)\n",
    "axes[2].set_ylabel(\"% same gene\")\n",
    "axes[2].set_xlabel(\"Distance to gene in human (bp)\")\n",
    "axes[2].set_title(f\"Conservation rate by distance ({example_species})\\n(gene-symbol peaks only)\")\n",
    "for i, (pct, n) in enumerate(zip(bin_stats[\"mean\"], bin_stats[\"count\"])):\n",
    "    axes[2].text(i, pct * 100 + 1, f\"n={n:,}\", ha=\"center\", fontsize=6, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = os.path.join(OUTPUT_DIR, \"gene_conservation_liftback.png\")\n",
    "plt.savefig(plot_file, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nSaved plot: {plot_file}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d96c5",
   "metadata": {},
   "source": [
    "## Master Annotation Table\n",
    "\n",
    "Comprehensive per-peak annotation with:\n",
    "- **Coordinates** in each species (from liftback for unified, original for species-specific)\n",
    "- **Nearest gene** and **distance to gene** in each species\n",
    "- **Binary detection columns** (`Human_det`, `Bonobo_det`, ...) for UpSet plotting\n",
    "- **Peak type**: `unified`, `human_specific`, or `{species}_specific`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load master annotation (produced by pipeline Step 7)\n",
    "master_file = os.path.join(OUTPUT_DIR, \"07_master_annotation\", \"master_annotation.tsv\")\n",
    "\n",
    "if os.path.exists(master_file):\n",
    "    master = pd.read_csv(master_file, sep=\"\\t\", index_col=\"peak_id\")\n",
    "    print(f\"Master annotation: {len(master):,} peaks x {len(master.columns)} columns\")\n",
    "    print(f\"\\nPeak types:\\n{master['peak_type'].value_counts()}\")\n",
    "\n",
    "    # Detection summary\n",
    "    det_cols = [c for c in master.columns if c.endswith(\"_det\")]\n",
    "    print(f\"\\nDetection summary:\")\n",
    "    for c in det_cols:\n",
    "        print(f\"  {c}: {master[c].sum():,} peaks\")\n",
    "\n",
    "    # Number of species per peak\n",
    "    print(f\"\\nPeaks by number of species detected:\")\n",
    "    print(master[\"n_species\"].value_counts().sort_index())\n",
    "\n",
    "    # Show a few rows with key columns\n",
    "    show_cols = [\"peak_type\", \"n_species\"] + det_cols\n",
    "    print(f\"\\nSample rows:\")\n",
    "    display(master[show_cols].head(10))\n",
    "else:\n",
    "    print(f\"Master annotation not found at {master_file}\")\n",
    "    print(\"Run the pipeline first (it now includes Steps 7 and 8)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UpSet-style visualization of species detection patterns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if os.path.exists(master_file):\n",
    "    master = pd.read_csv(master_file, sep=\"\\t\", index_col=\"peak_id\")\n",
    "\n",
    "    det_cols = [c for c in master.columns if c.endswith(\"_det\")]\n",
    "    species_names = [c.replace(\"_det\", \"\") for c in det_cols]\n",
    "\n",
    "    # Create detection pattern strings\n",
    "    master[\"pattern\"] = master[det_cols].apply(\n",
    "        lambda row: \",\".join([s for s, v in zip(species_names, row) if v == 1]),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    pattern_counts = master[\"pattern\"].value_counts().head(20)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    pattern_counts.plot(kind=\"barh\", ax=ax, color=\"steelblue\")\n",
    "    ax.set_xlabel(\"Number of peaks\")\n",
    "    ax.set_ylabel(\"Species combination\")\n",
    "    ax.set_title(\"Top 20 Species Detection Patterns (for UpSet plotting)\")\n",
    "    ax.invert_yaxis()\n",
    "    for i, v in enumerate(pattern_counts):\n",
    "        ax.text(v + 50, i, f\"{v:,}\", va=\"center\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Gene distance comparison across species\n",
    "    gene_dist_cols = [c for c in master.columns if c.endswith(\"_gene_dist\")]\n",
    "    if gene_dist_cols:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        for col in gene_dist_cols:\n",
    "            sp = col.replace(\"_gene_dist\", \"\")\n",
    "            vals = master[col].dropna()\n",
    "            if len(vals) > 0:\n",
    "                vals_kb = vals[vals >= 0] / 1000\n",
    "                ax.hist(vals_kb.clip(upper=500), bins=100, alpha=0.5, label=f\"{sp} (n={len(vals):,})\")\n",
    "        ax.set_xlabel(\"Distance to nearest gene (kb)\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.set_title(\"Distance to Nearest Gene by Species\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9a9eb",
   "metadata": {},
   "source": [
    "## Cross-Mapping Species-Specific Peaks\n",
    "\n",
    "Species-specific peaks are peaks that could not be lifted back from the unified hg38 consensus to the native species coordinates. Routing them through hg38 again would be pointless since they already failed that route.\n",
    "\n",
    "**Strategy:** Instead, we use **direct inter-species chain files** from UCSC, chaining through intermediate assemblies where needed. For example:\n",
    "- Bonobo(panPan2) -> Chimp(panTro5): `panPan2 -> panTro4 -> panTro5` (2 hops)\n",
    "- Gorilla(gorGor4) -> Bonobo(panPan2): `gorGor4 -> gorGor5 -> panPan2` (2 hops)\n",
    "- Macaque(rheMac10) -> Gorilla(gorGor4): `rheMac10 -> panTro6 -> panTro5 -> gorGor5 -> gorGor4` (4 hops)\n",
    "\n",
    "This tells us whether a \"species-specific\" peak actually has a corresponding open chromatin region in another species, via an alternative liftover path that bypasses hg38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cross-mapping results (produced by pipeline Step 8)\n",
    "cross_map_file = os.path.join(OUTPUT_DIR, \"08_cross_mapping\", \"species_specific_cross_mapping.tsv\")\n",
    "cross_matrix_file = os.path.join(OUTPUT_DIR, \"08_cross_mapping\", \"cross_mapping_matrix_pct.tsv\")\n",
    "cross_counts_file = os.path.join(OUTPUT_DIR, \"08_cross_mapping\", \"cross_mapping_matrix_counts.tsv\")\n",
    "\n",
    "if os.path.exists(cross_map_file):\n",
    "    cross_map = pd.read_csv(cross_map_file, sep=\"\\t\")\n",
    "    print(f\"Cross-mapping results: {len(cross_map)} source-target pairs\\n\")\n",
    "\n",
    "    # Show summary with route info\n",
    "    show_cols = [\"source\", \"target\", \"source_specific\", \"n_hops\",\n",
    "                 \"lifted_to_target\", \"overlap_target_peaks\", \"pct_overlap\"]\n",
    "    display(cross_map[show_cols])\n",
    "\n",
    "    # Heatmap of cross-mapping percentages\n",
    "    if os.path.exists(cross_matrix_file):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        matrix = pd.read_csv(cross_matrix_file, sep=\"\\t\", index_col=0)\n",
    "        print(f\"\\nCross-mapping matrix (% of source-specific peaks overlapping target):\")\n",
    "        display(matrix.round(1))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        # Percentage heatmap\n",
    "        ax = axes[0]\n",
    "        im = ax.imshow(matrix.values, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "        ax.set_xticks(range(len(matrix.columns)))\n",
    "        ax.set_xticklabels(matrix.columns, rotation=45, ha=\"right\")\n",
    "        ax.set_yticks(range(len(matrix.index)))\n",
    "        ax.set_yticklabels(matrix.index)\n",
    "        ax.set_xlabel(\"Target species\")\n",
    "        ax.set_ylabel(\"Source species (species-specific peaks)\")\n",
    "        ax.set_title(\"% of Species-Specific Peaks\\nOverlapping Target Peaks\")\n",
    "        for i in range(len(matrix.index)):\n",
    "            for j in range(len(matrix.columns)):\n",
    "                val = matrix.values[i, j]\n",
    "                color = \"white\" if val > 50 else \"black\"\n",
    "                ax.text(j, i, f\"{val:.1f}%\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "        plt.colorbar(im, ax=ax, label=\"% overlap\")\n",
    "\n",
    "        # Count heatmap\n",
    "        if os.path.exists(cross_counts_file):\n",
    "            counts = pd.read_csv(cross_counts_file, sep=\"\\t\", index_col=0)\n",
    "            ax2 = axes[1]\n",
    "            im2 = ax2.imshow(counts.values, cmap=\"Blues\", aspect=\"auto\")\n",
    "            ax2.set_xticks(range(len(counts.columns)))\n",
    "            ax2.set_xticklabels(counts.columns, rotation=45, ha=\"right\")\n",
    "            ax2.set_yticks(range(len(counts.index)))\n",
    "            ax2.set_yticklabels(counts.index)\n",
    "            ax2.set_xlabel(\"Target species\")\n",
    "            ax2.set_ylabel(\"Source species\")\n",
    "            ax2.set_title(\"Absolute Count of Overlapping Peaks\")\n",
    "            for i in range(len(counts.index)):\n",
    "                for j in range(len(counts.columns)):\n",
    "                    val = int(counts.values[i, j])\n",
    "                    color = \"white\" if val > counts.values.max() * 0.6 else \"black\"\n",
    "                    ax2.text(j, i, f\"{val:,}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "            plt.colorbar(im2, ax=ax2, label=\"count\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Cross-mapping not found at {cross_map_file}\")\n",
    "    print(\"Run the pipeline first (it now includes Steps 7 and 8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdb10a",
   "metadata": {},
   "source": [
    "## 8. Build Combined BED File\n",
    "\n",
    "Write a single BED file with all peak categories: unified (hg38), human-specific (hg38), and species-specific (native coords). Also write per-species BED files with liftback unified + species-specific peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Build combined BED file (all peak categories)\n",
    "# =============================================================================\n",
    "combined_dir = os.path.join(OUTPUT_DIR, \"07_combined\")\n",
    "os.makedirs(combined_dir, exist_ok=True)\n",
    "\n",
    "combined_bed = os.path.join(combined_dir, \"all_peaks_combined.bed\")\n",
    "\n",
    "with open(combined_bed, 'w') as fout:\n",
    "    # Header as a comment\n",
    "    fout.write(\"#chr\\tstart\\tend\\tpeak_id\\tcategory\\tgenome_assembly\\n\")\n",
    "\n",
    "    # 1. Unified peaks (hg38)\n",
    "    n_unified = 0\n",
    "    with open(results[\"output_files\"][\"unified_consensus\"]) as fin:\n",
    "        for line in fin:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\tunified\\thg38\\n\")\n",
    "                n_unified += 1\n",
    "\n",
    "    # 2. Human-specific peaks (hg38)\n",
    "    n_human_spec = 0\n",
    "    with open(results[\"output_files\"][\"human_specific\"]) as fin:\n",
    "        for line in fin:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\thuman_specific\\thg38\\n\")\n",
    "                n_human_spec += 1\n",
    "\n",
    "    # 3. Species-specific peaks (native coordinates)\n",
    "    assembly_map = {\n",
    "        \"Bonobo\": \"panPan2\", \"Chimpanzee\": \"panTro5\", \"Gorilla\": \"gorGor4\",\n",
    "        \"Macaque\": \"rheMac10\", \"Marmoset\": \"calJac1\",\n",
    "    }\n",
    "    n_sp_spec = 0\n",
    "    for species in SPECIES_BEDS:\n",
    "        key = f\"species_specific_{species}\"\n",
    "        if key in results[\"output_files\"] and os.path.exists(results[\"output_files\"][key]):\n",
    "            assembly = assembly_map.get(species, species)\n",
    "            with open(results[\"output_files\"][key]) as fin:\n",
    "                for line in fin:\n",
    "                    if line.strip() and not line.startswith('#'):\n",
    "                        parts = line.strip().split('\\t')\n",
    "                        fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\t{species.lower()}_specific\\t{assembly}\\n\")\n",
    "                        n_sp_spec += 1\n",
    "\n",
    "print(f\"Combined BED written: {combined_bed}\")\n",
    "print(f\"  Unified:          {n_unified:,}\")\n",
    "print(f\"  Human-specific:   {n_human_spec:,}\")\n",
    "print(f\"  Species-specific: {n_sp_spec:,}\")\n",
    "print(f\"  Total:            {n_unified + n_human_spec + n_sp_spec:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Per-species BED files (liftback unified + species-specific, in native coords)\n",
    "# =============================================================================\n",
    "print(f\"\\nPer-species complete peak sets:\")\n",
    "\n",
    "for species in SPECIES_BEDS:\n",
    "    sp_combined = os.path.join(combined_dir, f\"all_peaks_{species}.bed\")\n",
    "    n = 0\n",
    "\n",
    "    with open(sp_combined, 'w') as fout:\n",
    "        # Liftback unified peaks\n",
    "        liftback_key = f\"liftback_{species}\"\n",
    "        if liftback_key in results[\"output_files\"]:\n",
    "            liftback_file = results[\"output_files\"][liftback_key]\n",
    "            if os.path.exists(liftback_file):\n",
    "                with open(liftback_file) as fin:\n",
    "                    for line in fin:\n",
    "                        if line.strip() and not line.startswith('#'):\n",
    "                            parts = line.strip().split('\\t')\n",
    "                            # liftback may have: chr, start, end, peak_id, source_coords\n",
    "                            fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\n\")\n",
    "                            n += 1\n",
    "\n",
    "        # Species-specific peaks\n",
    "        sp_key = f\"species_specific_{species}\"\n",
    "        if sp_key in results[\"output_files\"]:\n",
    "            sp_file = results[\"output_files\"][sp_key]\n",
    "            if os.path.exists(sp_file):\n",
    "                with open(sp_file) as fin:\n",
    "                    for line in fin:\n",
    "                        if line.strip() and not line.startswith('#'):\n",
    "                            parts = line.strip().split('\\t')\n",
    "                            fout.write(f\"{parts[0]}\\t{parts[1]}\\t{parts[2]}\\t{parts[3]}\\n\")\n",
    "                            n += 1\n",
    "\n",
    "    print(f\"  {species}: {n:,} peaks -> {sp_combined}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7f339",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Summary statistics and validation\n",
    "# =============================================================================\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Check no duplicate peak IDs\n",
    "all_ids = annot_df[\"peak_id\"].tolist()\n",
    "n_unique = len(set(all_ids))\n",
    "n_total = len(all_ids)\n",
    "print(f\"\\nPeak ID uniqueness: {n_unique:,} unique / {n_total:,} total\", end=\"\")\n",
    "if n_unique == n_total:\n",
    "    print(\" -- OK\")\n",
    "else:\n",
    "    dup_ids = annot_df[annot_df[\"peak_id\"].duplicated(keep=False)][\"peak_id\"].unique()\n",
    "    print(f\" -- WARNING: {n_total - n_unique} duplicates!\")\n",
    "    print(f\"  Duplicate IDs: {list(dup_ids[:10])}\")\n",
    "\n",
    "# Helper to extract input count from liftover result dict\n",
    "def _get_liftover_counts(res):\n",
    "    \"\"\"Extract (input, lifted) from a liftover result dict.\"\"\"\n",
    "    lifted = res.get(\"lifted\", 0)\n",
    "    # liftover_peaks returns lifted + unmapped; two_step returns \"original\"\n",
    "    if \"original\" in res:\n",
    "        inp = res[\"original\"]\n",
    "    elif \"unmapped\" in res:\n",
    "        inp = lifted + res[\"unmapped\"]\n",
    "    else:\n",
    "        inp = lifted  # pre-lifted (no unmapped info) -- show lifted as input\n",
    "    return inp, lifted\n",
    "\n",
    "# 2. Liftover success rates\n",
    "print(f\"\\nLiftover success rates (species -> hg38):\")\n",
    "print(f\"  {'Species':<15s} {'Input':>10s} {'Lifted':>10s} {'Rate':>8s}  {'Note'}\")\n",
    "print(f\"  {'-'*58}\")\n",
    "for species, res in results[\"lift_to_human\"].items():\n",
    "    inp, lifted = _get_liftover_counts(res)\n",
    "    rate = lifted / inp * 100 if inp > 0 else 0\n",
    "    note = \"(pre-lifted)\" if res.get(\"source\") == \"pre_lifted\" else \"\"\n",
    "    print(f\"  {species:<15s} {inp:>10,} {lifted:>10,} {rate:>7.1f}%  {note}\")\n",
    "\n",
    "print(f\"\\nLiftback success rates (hg38 -> species):\")\n",
    "print(f\"  {'Species':<15s} {'Input':>10s} {'Lifted':>10s} {'Rate':>8s}\")\n",
    "print(f\"  {'-'*48}\")\n",
    "for species, res in results[\"lift_back\"].items():\n",
    "    inp, lifted = _get_liftover_counts(res)\n",
    "    rate = lifted / inp * 100 if inp > 0 else 0\n",
    "    print(f\"  {species:<15s} {inp:>10,} {lifted:>10,} {rate:>7.1f}%\")\n",
    "\n",
    "# 3. Species detection distribution plot\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SPECIES DETECTION IN UNIFIED PEAKS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart: peaks per n_species\n",
    "n_species_counts = unified_df[\"n_species\"].value_counts().sort_index()\n",
    "axes[0].bar(n_species_counts.index, n_species_counts.values, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[0].set_xlabel(\"Number of species detected\")\n",
    "axes[0].set_ylabel(\"Number of peaks\")\n",
    "axes[0].set_title(\"Distribution of species detection\\n(unified consensus)\")\n",
    "for x, y in zip(n_species_counts.index, n_species_counts.values):\n",
    "    axes[0].text(x, y, f\"{y:,}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "# Bar chart: peak category counts\n",
    "categories = {\"Unified\": len(unified_df), \"Human-specific\": len(hs_df)}\n",
    "for sp, n in species_specific_counts.items():\n",
    "    categories[f\"{sp}-specific\"] = n\n",
    "\n",
    "cats = list(categories.keys())\n",
    "vals = list(categories.values())\n",
    "colors = [\"steelblue\"] + [\"firebrick\"] + [\"seagreen\"] * len(species_specific_counts)\n",
    "axes[1].barh(cats, vals, color=colors, edgecolor=\"white\")\n",
    "axes[1].set_xlabel(\"Number of peaks\")\n",
    "axes[1].set_title(\"Peak counts by category\")\n",
    "for y_pos, v in enumerate(vals):\n",
    "    axes[1].text(v, y_pos, f\"  {v:,}\", ha=\"left\", va=\"center\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_file = os.path.join(OUTPUT_DIR, \"peak_summary.png\")\n",
    "plt.savefig(plot_file, dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nSaved summary plot: {plot_file}\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Save summary stats\n",
    "summary_file = os.path.join(OUTPUT_DIR, \"pipeline_summary.txt\")\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(\"Cross-Species Consensus Pipeline v2 -- Summary\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    f.write(f\"Unified consensus peaks: {len(unified_df):,}\\n\")\n",
    "    f.write(f\"Human-specific peaks:    {len(hs_df):,}\\n\")\n",
    "    for sp, n in species_specific_counts.items():\n",
    "        f.write(f\"{sp}-specific peaks:  {n:,}\\n\")\n",
    "    f.write(f\"\\nSpecies detection distribution (unified):\\n\")\n",
    "    for n_sp, count in n_species_counts.items():\n",
    "        f.write(f\"  {n_sp} species: {count:,} peaks\\n\")\n",
    "    f.write(f\"\\nAnnotation file: {annotation_file}\\n\")\n",
    "    f.write(f\"Combined BED:    {combined_bed}\\n\")\n",
    "\n",
    "print(f\"Saved summary: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
