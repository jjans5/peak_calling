{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c44564c",
   "metadata": {},
   "source": [
    "# ATAC-seq Peak Calling Pipeline\n",
    "\n",
    "A reproducible workflow for:\n",
    "1. Converting fragment files to Tn5 cut-sites\n",
    "2. Calling peaks with MACS3\n",
    "3. Lifting over peaks to human genome (hg38)\n",
    "4. Generating consensus peaks\n",
    "5. Creating BigWig files\n",
    "\n",
    "**Note:** This notebook uses the `config/config.yaml` file for default paths.\n",
    "For more flexibility with custom paths, use `02_flexible_workflow.ipynb`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647a2cf",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import modules and load configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18871463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add src to path\n",
    "PIPELINE_DIR = Path(os.getcwd()).parent if 'notebooks' in os.getcwd() else Path(os.getcwd())\n",
    "sys.path.insert(0, str(PIPELINE_DIR))\n",
    "\n",
    "# Import pipeline modules\n",
    "from src.peak_calling import (\n",
    "    process_all_fragments,\n",
    "    run_peak_calling,\n",
    "    EFFECTIVE_GENOME_SIZES,\n",
    "    DEFAULT_MACS3_PARAMS,\n",
    ")\n",
    "from src.consensus import (\n",
    "    get_consensus_peaks,\n",
    "    load_narrowpeaks,\n",
    "    harmonize_chromosomes,\n",
    ")\n",
    "from src.liftover import liftover_peaks, print_chain_info, get_chain_file, DEFAULT_CHAIN_DIR\n",
    "from src.bigwig import create_bigwig, process_all_fragments_to_bigwig\n",
    "from src.utils import get_chromsizes, save_parameters, ensure_dir\n",
    "from src.visualization import plot_peak_distribution, plot_consensus_summary, plot_peak_counts_report\n",
    "\n",
    "print(f\"Pipeline directory: {PIPELINE_DIR}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from config.yaml\n",
    "config_path = PIPELINE_DIR / \"config\" / \"config.yaml\"\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded from:\", config_path)\n",
    "print(f\"\\nüìã Species: {config['species']}\")\n",
    "print(f\"üìã MACS3 qvalue: {config['macs3']['params']['qvalue']}\")\n",
    "print(f\"üìã Peak half-width: {config['consensus']['peak_half_width']}bp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chain file information\n",
    "print_chain_info(config['paths']['chain_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb02c24",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set parameters for the analysis. These are loaded from `config.yaml` but can be overridden here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd0ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Loaded from config.yaml, override here if needed\n",
    "# =============================================================================\n",
    "\n",
    "# Species to process\n",
    "SPECIES = config['species']  # Options: \"Gorilla\", \"Human\", \"Chimpanzee\", \"Bonobo\", \"Macaque\", \"Marmoset\"\n",
    "\n",
    "# Input paths\n",
    "BASE_DIR = config['paths']['base_dir']\n",
    "FRAGMENTS_INPUT_DIR = config['paths']['fragments_dir'].replace('{species}', SPECIES)\n",
    "\n",
    "# Output paths (relative to pipeline directory)\n",
    "OUTPUT_BASE = PIPELINE_DIR / \"output\" / SPECIES\n",
    "CUTSITES_DIR = OUTPUT_BASE / \"cutsites\"\n",
    "PEAKS_DIR = OUTPUT_BASE / \"peaks\"\n",
    "LIFTED_DIR = OUTPUT_BASE / \"lifted_hg38\"\n",
    "CONSENSUS_DIR = OUTPUT_BASE / \"consensus\"\n",
    "BIGWIG_DIR = OUTPUT_BASE / \"bigwigs\"\n",
    "\n",
    "# Reference files\n",
    "CHROMSIZES_DIR = config['paths']['chromsizes_dir']\n",
    "CHAIN_DIR = config['paths']['chain_dir']\n",
    "\n",
    "# MACS3 executable\n",
    "MACS3_PATH = config['macs3']['executable']\n",
    "\n",
    "# Processing parameters\n",
    "CUTSITE_WORKERS = config['parallel']['cutsite_workers']\n",
    "MACS3_WORKERS = config['parallel']['macs3_workers']\n",
    "LIFTOVER_WORKERS = config['parallel']['liftover_workers']\n",
    "\n",
    "# MACS3 parameters\n",
    "MACS3_PARAMS = config['macs3']['params']\n",
    "\n",
    "# Consensus parameters\n",
    "PEAK_HALF_WIDTH = config['consensus']['peak_half_width']\n",
    "Q_VALUE_THRESHOLD = config['consensus']['q_value_threshold']\n",
    "MIN_PEAKS_PER_SAMPLE = config['consensus']['min_peaks_per_sample']\n",
    "\n",
    "# Create output directories\n",
    "for d in [CUTSITES_DIR, PEAKS_DIR, LIFTED_DIR, CONSENSUS_DIR, BIGWIG_DIR]:\n",
    "    ensure_dir(str(d))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CONFIGURATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Species: {SPECIES}\")\n",
    "print(f\"Genome size: {EFFECTIVE_GENOME_SIZES.get(SPECIES, 'Unknown'):,}\")\n",
    "print(f\"\\nInput:\")\n",
    "print(f\"  Fragments: {FRAGMENTS_INPUT_DIR}\")\n",
    "print(f\"  Chromsizes: {CHROMSIZES_DIR}\")\n",
    "print(f\"  Chain files: {CHAIN_DIR}\")\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  Base: {OUTPUT_BASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d97059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display MACS3 parameters\n",
    "print(\"MACS3 Parameters:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in MACS3_PARAMS.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e006e6",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Convert Fragments to Cut-Sites\n",
    "\n",
    "Convert paired-end fragment files to single-nucleotide Tn5 cut-site BED files.\n",
    "\n",
    "For each fragment `(chr, start, end)`, we extract:\n",
    "- **5' cut site**: `(chr, start, start+1)` with `+` strand\n",
    "- **3' cut site**: `(chr, end-1, end)` with `-` strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2307830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if input directory exists\n",
    "if os.path.exists(FRAGMENTS_INPUT_DIR):\n",
    "    print(f\"‚úÖ Fragment directory found: {FRAGMENTS_INPUT_DIR}\")\n",
    "    print(f\"   Files: {len(list(Path(FRAGMENTS_INPUT_DIR).glob('*.tsv.gz')))} .tsv.gz files\")\n",
    "else:\n",
    "    print(f\"‚ùå Fragment directory NOT found: {FRAGMENTS_INPUT_DIR}\")\n",
    "    print(\"   Please check your config.yaml or set FRAGMENTS_INPUT_DIR manually above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fragment to cut-site conversion (uncomment to run)\n",
    "# print(f\"Converting fragments to cut-sites...\")\n",
    "# print(f\"Input: {FRAGMENTS_INPUT_DIR}\")\n",
    "# print(f\"Output: {CUTSITES_DIR}\")\n",
    "# print()\n",
    "\n",
    "# cutsite_results = process_all_fragments(\n",
    "#     input_dir=FRAGMENTS_INPUT_DIR,\n",
    "#     output_dir=str(CUTSITES_DIR),\n",
    "#     max_workers=CUTSITE_WORKERS,\n",
    "# )\n",
    "\n",
    "print(\"‚è∏Ô∏è Fragment conversion step - uncomment the code above to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b1afae",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: MACS3 Peak Calling\n",
    "\n",
    "Run MACS3 peak calling on the cut-site BED files.\n",
    "\n",
    "**Output files per sample:**\n",
    "- `*_peaks.narrowPeak`: BED6+4 format peak calls\n",
    "- `*_peaks.xls`: Spreadsheet with peak info\n",
    "- `*_summits.bed`: Peak summit positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MACS3 peak calling (uncomment to run)\n",
    "# print(f\"Running MACS3 peak calling...\")\n",
    "# print(f\"Input: {CUTSITES_DIR}\")\n",
    "# print(f\"Output: {PEAKS_DIR}\")\n",
    "# print()\n",
    "\n",
    "# peak_results = run_peak_calling(\n",
    "#     species=SPECIES,\n",
    "#     frag_dir=str(CUTSITES_DIR),\n",
    "#     out_dir=str(PEAKS_DIR),\n",
    "#     macs3_path=MACS3_PATH,\n",
    "#     max_workers=MACS3_WORKERS,\n",
    "#     params=MACS3_PARAMS,\n",
    "# )\n",
    "\n",
    "print(\"‚è∏Ô∏è Peak calling step - uncomment the code above to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990a4ae",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Liftover to Human Genome (hg38)\n",
    "\n",
    "Lift peaks to hg38 for cross-species comparison.\n",
    "\n",
    "**Note:** Skip this step for Human samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chain file info for current species\n",
    "if SPECIES != \"Human\":\n",
    "    chain_file = get_chain_file(SPECIES, CHAIN_DIR)\n",
    "else:\n",
    "    print(\"Species is Human - no liftover needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run liftover (uncomment to run)\n",
    "# if SPECIES != \"Human\":\n",
    "#     print(f\"Lifting over peaks to hg38...\")\n",
    "#     print(f\"Chain file: {chain_file}\")\n",
    "#     print()\n",
    "    \n",
    "#     # Liftover each narrowPeak file\n",
    "#     narrowpeak_files = list(PEAKS_DIR.glob(\"*_peaks.narrowPeak\"))\n",
    "    \n",
    "#     liftover_results = []\n",
    "#     for np_file in narrowpeak_files:\n",
    "#         output_file = LIFTED_DIR / np_file.name.replace(\".narrowPeak\", \".hg38.bed\")\n",
    "#         result = liftover_peaks(\n",
    "#             input_bed=str(np_file),\n",
    "#             output_bed=str(output_file),\n",
    "#             chain_file=chain_file,\n",
    "#         )\n",
    "#         print(result[\"message\"])\n",
    "#         liftover_results.append(result)\n",
    "    \n",
    "#     total_lifted = sum(r[\"lifted\"] for r in liftover_results)\n",
    "#     total_unmapped = sum(r[\"unmapped\"] for r in liftover_results)\n",
    "#     print(f\"\\nTotal lifted: {total_lifted:,}, unmapped: {total_unmapped:,}\")\n",
    "\n",
    "print(\"‚è∏Ô∏è Liftover step - uncomment the code above to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98929498",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Consensus Peak Calling\n",
    "\n",
    "Generate consensus peaks by:\n",
    "1. Loading and filtering narrowPeak files\n",
    "2. Extending peaks from summit by half-width\n",
    "3. Normalizing scores (CPM)\n",
    "4. Iteratively resolving overlaps by selecting highest-scoring peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load narrowPeak files (uncomment to run)\n",
    "# print(\"Loading narrowPeak files...\")\n",
    "# narrow_peaks_dict = load_narrowpeaks(\n",
    "#     peak_dir=str(PEAKS_DIR),\n",
    "#     q_value_threshold=Q_VALUE_THRESHOLD,\n",
    "#     min_peaks_per_sample=MIN_PEAKS_PER_SAMPLE,\n",
    "# )\n",
    "# print(f\"\\nLoaded {len(narrow_peaks_dict)} samples\")\n",
    "\n",
    "print(\"‚è∏Ô∏è Consensus peak calling step - uncomment the code above to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe5146",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Generate BigWig Files\n",
    "\n",
    "Create genome coverage bigWig files from fragment files for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chromsizes file\n",
    "chromsizes_file = os.path.join(CHROMSIZES_DIR, config['chromsizes_files'].get(SPECIES, \"\"))\n",
    "print(f\"Chromsizes file: {chromsizes_file}\")\n",
    "print(f\"Exists: {os.path.exists(chromsizes_file)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BigWig files (uncomment to run)\n",
    "# print(\"Generating BigWig files...\")\n",
    "# bigwig_results = process_all_fragments_to_bigwig(\n",
    "#     input_dir=FRAGMENTS_INPUT_DIR,\n",
    "#     output_dir=str(BIGWIG_DIR),\n",
    "#     chrom_sizes_file=chromsizes_file,\n",
    "#     pattern=\"*.tsv.gz\",\n",
    "#     cut_sites=True,\n",
    "#     normalize=True,\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "print(\"‚è∏Ô∏è BigWig generation step - uncomment the code above to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fc7c5",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "Display configuration and output locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8664d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSpecies: {SPECIES}\")\n",
    "print(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nInput directory: {FRAGMENTS_INPUT_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_BASE}\")\n",
    "print(f\"\\nChain file directory: {CHAIN_DIR}\")\n",
    "print(f\"Chromsizes directory: {CHROMSIZES_DIR}\")\n",
    "print(f\"\\nTo run the full pipeline, uncomment the code cells above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
