{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c8f40f",
   "metadata": {},
   "source": [
    "# Quantification Over Unified Peaks\n",
    "\n",
    "Quantify fragment files, bigWigs, or Tn5 insertion BEDs over the cross-species\n",
    "unified peak set (or any BED peak file).\n",
    "\n",
    "## Workflow\n",
    "1. **Load** the unified consensus peak set (from `Cross_species_consensus` notebook)\n",
    "2. **Discover** fragment / bigWig / Tn5 files per species\n",
    "3. **Quantify** – single file or full matrix\n",
    "4. **Save / load** results in feather, parquet, or TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4bb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Ensure bedtools is on PATH (lives in scenicplus conda env)\n",
    "bedtools_dir = '/cluster/project/treutlein/jjans/software/miniforge3/envs/scenicplus/bin'\n",
    "os.environ['PATH'] = bedtools_dir + ':' + os.environ.get('PATH', '')\n",
    "\n",
    "# Add atac_pipeline to path\n",
    "sys.path.insert(0, '/cluster/home/jjanssens/jjans/analysis/adult_intestine/peaks/peak_calling/atac_pipeline')\n",
    "\n",
    "from src.quantification import quantify, quantify_matrix, save_matrix, load_matrix\n",
    "\n",
    "print(\"Quantification module loaded\")\n",
    "print(\"   quantify()        - single file over peaks\")\n",
    "print(\"   quantify_matrix() - multiple files -> peaks x samples matrix\")\n",
    "print(\"   save/load_matrix  - feather / parquet / tsv I/O\")\n",
    "bt = subprocess.run('bedtools --version', shell=True, capture_output=True, text=True)\n",
    "print(f\"   {bt.stdout.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d122d69",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1be08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "\n",
    "BASE_PATH = \"/cluster/project/treutlein/USERS/jjans\"\n",
    "CROSS_SPECIES_DIR = f\"{BASE_PATH}/analysis/adult_intestine/peaks/cross_species_consensus\"\n",
    "\n",
    "# Species\n",
    "SPECIES_LIST = [\"Bonobo\", \"Chimpanzee\", \"Gorilla\", \"Macaque\", \"Marmoset\", \"Human\"]\n",
    "\n",
    "# Output\n",
    "QUANT_OUTPUT_DIR = os.path.join(CROSS_SPECIES_DIR, \"04_quantification\")\n",
    "os.makedirs(QUANT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {QUANT_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0c5e5",
   "metadata": {},
   "source": [
    "## Load Peak Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unified consensus peak file (hg38) produced by Cross_species_consensus notebook\n",
    "UNIFIED_PEAKS = os.path.join(\n",
    "    CROSS_SPECIES_DIR, \"02_merged_consensus\", \"unified_consensus_hg38_with_ids.bed\"\n",
    ")\n",
    "\n",
    "# Per-species liftback peaks (in each species' own coordinates)\n",
    "LIFTBACK_DIR = os.path.join(CROSS_SPECIES_DIR, \"03_lifted_back\")\n",
    "\n",
    "SPECIES_PEAK_FILES = {}\n",
    "for species in SPECIES_LIST:\n",
    "    if species == \"Human\":\n",
    "        peak_file = UNIFIED_PEAKS  # already hg38\n",
    "    else:\n",
    "        peak_file = os.path.join(LIFTBACK_DIR, f\"unified_consensus_{species}.bed\")\n",
    "    \n",
    "    if os.path.exists(peak_file):\n",
    "        with open(peak_file) as f:\n",
    "            n = sum(1 for line in f if line.strip())\n",
    "        SPECIES_PEAK_FILES[species] = peak_file\n",
    "        print(f\"{species:<15} {n:>8,} peaks  {peak_file}\")\n",
    "    else:\n",
    "        print(f\"{species:<15} NOT FOUND: {peak_file}\")\n",
    "\n",
    "print(f\"Peak files available for {len(SPECIES_PEAK_FILES)}/{len(SPECIES_LIST)} species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5471b4",
   "metadata": {},
   "source": [
    "## Discover Fragment Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71817e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect fragment files for each species\n",
    "# Adjust FRAGMENT_DIR_TEMPLATE if your directory layout differs\n",
    "\n",
    "FRAGMENT_DIR_TEMPLATE = f\"{BASE_PATH}/analysis/adult_intestine/peaks/fragment_files/{{species}}\"\n",
    "FRAGMENT_PATTERN = \"*.tsv.gz\"\n",
    "\n",
    "SPECIES_FRAGMENTS = {}\n",
    "\n",
    "print(\"Scanning for fragment files:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for species in SPECIES_LIST:\n",
    "    frag_dir = FRAGMENT_DIR_TEMPLATE.format(species=species)\n",
    "    \n",
    "    if not os.path.isdir(frag_dir):\n",
    "        print(f\"{species:<15} directory not found: {frag_dir}\")\n",
    "        continue\n",
    "    \n",
    "    files = sorted(glob.glob(os.path.join(frag_dir, FRAGMENT_PATTERN)))\n",
    "    \n",
    "    if files:\n",
    "        SPECIES_FRAGMENTS[species] = files\n",
    "        print(f\"{species:<15} {len(files):>4} files   ({frag_dir})\")\n",
    "    else:\n",
    "        print(f\"{species:<15} 0 files matching {FRAGMENT_PATTERN} in {frag_dir}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "total_files = sum(len(v) for v in SPECIES_FRAGMENTS.values())\n",
    "print(f\"{total_files} total fragment files across {len(SPECIES_FRAGMENTS)} species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview files for one species\n",
    "PREVIEW_SPECIES = SPECIES_LIST[0]  # change as needed\n",
    "\n",
    "if PREVIEW_SPECIES in SPECIES_FRAGMENTS:\n",
    "    files = SPECIES_FRAGMENTS[PREVIEW_SPECIES]\n",
    "    print(f\"{PREVIEW_SPECIES} fragment files ({len(files)}):\")\n",
    "    for f in files[:10]:\n",
    "        print(f\"   {os.path.basename(f)}\")\n",
    "    if len(files) > 10:\n",
    "        print(f\"   ... and {len(files) - 10} more\")\n",
    "else:\n",
    "    print(f\"No fragments found for {PREVIEW_SPECIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498acde6",
   "metadata": {},
   "source": [
    "## Single-File Quantification (test)\n",
    "\n",
    "Try one file to make sure everything works before running the full matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first species that has both peaks and fragments\n",
    "test_species = next(\n",
    "    (s for s in SPECIES_LIST if s in SPECIES_FRAGMENTS and s in SPECIES_PEAK_FILES),\n",
    "    None,\n",
    " )\n",
    "\n",
    "if test_species:\n",
    "    test_file = SPECIES_FRAGMENTS[test_species][0]\n",
    "    test_peaks = SPECIES_PEAK_FILES[test_species]\n",
    "    \n",
    "    print(f\"Testing with {test_species}\")\n",
    "    print(f\"   File:  {os.path.basename(test_file)}\")\n",
    "    print(f\"   Peaks: {os.path.basename(test_peaks)}\")\n",
    "    \n",
    "    result = quantify(\n",
    "        input_file=test_file,\n",
    "        peak_file=test_peaks,\n",
    "        input_type=\"fragments\",   # \"fragments\" | \"tn5\" | \"bigwig\"\n",
    "        method=\"cutsites\",        # \"coverage\" | \"cutsites\"  (fragments only)\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResult shape: {result.shape}\")\n",
    "    print(f\"   Non-zero peaks: {(result > 0).sum():,} / {len(result):,}\")\n",
    "    print(result.head(10))\n",
    "else:\n",
    "    print(\"No species has both peak file and fragment files. Check paths above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39409e1",
   "metadata": {},
   "source": [
    "## Build Quantification Matrix\n",
    "\n",
    "Run quantification for one or all species. The result is a **peaks \\u00d7 samples** matrix.\n",
    "\n",
    "All input types (`fragments`, `tn5`, `bigwig`) use the same interface — just change `input_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3dba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QUANTIFY ONE SPECIES ===\n",
    "\n",
    "SPECIES = \"Human\"  # Change to run a different species\n",
    "\n",
    "if SPECIES in SPECIES_FRAGMENTS and SPECIES in SPECIES_PEAK_FILES:\n",
    "    input_files = SPECIES_FRAGMENTS[SPECIES]\n",
    "    peak_file   = SPECIES_PEAK_FILES[SPECIES]\n",
    "    \n",
    "    print(f\"Quantifying {SPECIES}: {len(input_files)} files over {peak_file}\")\n",
    "    \n",
    "    matrix = quantify_matrix(\n",
    "        input_files=input_files,\n",
    "        peak_file=peak_file,\n",
    "        input_type=\"fragments\",       # \"fragments\" | \"tn5\" | \"bigwig\"\n",
    "        method=\"cutsites\",            # \"coverage\" | \"cutsites\"  (fragments only)\n",
    "        # stat=\"mean\",                # \"mean\"|\"sum\"|\"max\"|\"min\" (bigwig only)\n",
    "        n_workers=8,\n",
    "        name_pattern=r\"_fragments.*$\",  # regex to clean filenames\n",
    "        name_replacement=\"\",\n",
    "        output_file=os.path.join(QUANT_OUTPUT_DIR, f\"quantification_{SPECIES}\"),\n",
    "        output_format=\"feather\",       # \"feather\" | \"parquet\" | \"tsv\"\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMatrix shape: {matrix.shape}\")\n",
    "    print(matrix.head())\n",
    "else:\n",
    "    print(f\"Missing peaks or fragments for {SPECIES}. Check configuration above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === QUANTIFY ALL SPECIES ===\n",
    "# Loops over every species that has both fragments and peak files.\n",
    "# Each species gets its own output matrix.\n",
    "\n",
    "# for species in SPECIES_LIST:\n",
    "#     if species not in SPECIES_FRAGMENTS or species not in SPECIES_PEAK_FILES:\n",
    "#         print(f\"Skipping {species}: missing data\")\n",
    "#         continue\n",
    "#\n",
    "#     input_files = SPECIES_FRAGMENTS[species]\n",
    "#     peak_file   = SPECIES_PEAK_FILES[species]\n",
    "#\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"{species}: {len(input_files)} files\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#\n",
    "#     quantify_matrix(\n",
    "#         input_files=input_files,\n",
    "#         peak_file=peak_file,\n",
    "#         input_type=\"fragments\",\n",
    "#         method=\"cutsites\",\n",
    "#         n_workers=8,\n",
    "#         name_pattern=r\"_fragments.*$\",\n",
    "#         name_replacement=\"\",\n",
    "#         output_file=os.path.join(QUANT_OUTPUT_DIR, f\"quantification_{species}\"),\n",
    "#         output_format=\"feather\",\n",
    "#         verbose=True,\n",
    "#     )\n",
    "\n",
    "print(\"Uncomment the loop above to quantify all species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11091b",
   "metadata": {},
   "source": [
    "## Memory-Efficient Mode\n",
    "\n",
    "For very large numbers of files, use `chunk_size` to process in batches and avoid\n",
    "loading the full matrix in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MEMORY-EFFICIENT MODE ===\n",
    "# Processes N files at a time, writes intermediate chunks.\n",
    "\n",
    "# quantify_matrix(\n",
    "#     input_files=input_files,\n",
    "#     peak_file=peak_file,\n",
    "#     input_type=\"fragments\",\n",
    "#     method=\"cutsites\",\n",
    "#     n_workers=8,\n",
    "#     chunk_size=50,                # ← process 50 files at a time\n",
    "#     output_file=os.path.join(QUANT_OUTPUT_DIR, f\"quantification_large_{SPECIES}\"),\n",
    "#     output_format=\"parquet\",      # parquet recommended for large files\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "print(\"Uncomment and adjust chunk_size for large-scale quantification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95873b0",
   "metadata": {},
   "source": [
    "## Load and Inspect Saved Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55534a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved matrix (auto-detects feather / parquet / tsv from extension)\n",
    "# quant_df = load_matrix(os.path.join(QUANT_OUTPUT_DIR, f\"quantification_{SPECIES}.feather\"))\n",
    "# print(f\"Matrix shape: {quant_df.shape}\")\n",
    "# print(f\"   Peaks:   {quant_df.shape[0]:,}\")\n",
    "# print(f\"   Samples: {quant_df.shape[1]}\")\n",
    "# print(quant_df.head())\n",
    "\n",
    "print(\"Uncomment after running quantification to inspect results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a5700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all saved quantification files\n",
    "print(f\"Saved quantification files in {QUANT_OUTPUT_DIR}:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if os.path.isdir(QUANT_OUTPUT_DIR):\n",
    "    for f in sorted(os.listdir(QUANT_OUTPUT_DIR)):\n",
    "        fpath = os.path.join(QUANT_OUTPUT_DIR, f)\n",
    "        size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "        print(f\"   {f:<50} {size_mb:>8.1f} MB\")\n",
    "else:\n",
    "    print(\"   (no output directory yet)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
