{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48fa838",
   "metadata": {},
   "source": [
    "# Quantification Debug & Benchmark\n",
    "\n",
    "Test correctness, parallelization, RAM usage, and estimate time for full runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f0bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial RSS: 186 MB\n",
      "Peak file: unified_consensus_Bonobo.bed\n",
      "Fragment file: Adipocytes.fragments.tsv.gz (13.7 MB)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gzip, glob, time, resource\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '/cluster/home/jjanssens/jjans/analysis/adult_intestine/peaks/peak_calling/atac_pipeline')\n",
    "\n",
    "import importlib\n",
    "import src.quantification\n",
    "importlib.reload(src.quantification)\n",
    "from src.quantification import quantify, quantify_matrix, _build_peak_index, _count_1bp_hits\n",
    "from src.utils import load_peaks\n",
    "\n",
    "# Paths\n",
    "BASE = '/cluster/project/treutlein/USERS/jjans/analysis/adult_intestine/peaks'\n",
    "PEAK_FILE = f'{BASE}/cross_species_consensus/03_lifted_back/unified_consensus_Bonobo.bed'\n",
    "FRAG_FILE = f'{BASE}/fragment_files/Bonobo/Adipocytes.fragments.tsv.gz'  # small: ~14 MB\n",
    "HUMAN_PEAK_FILE = f'{BASE}/cross_species_consensus/02_merged_consensus/unified_consensus_hg38_with_ids.bed'\n",
    "HUMAN_FRAG_DIR = f'{BASE}/fragment_files/Human'\n",
    "\n",
    "def get_rss_mb():\n",
    "    \"\"\"Current resident set size in MB.\"\"\"\n",
    "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "\n",
    "print(f'Initial RSS: {get_rss_mb():.0f} MB')\n",
    "print(f'Peak file: {os.path.basename(PEAK_FILE)}')\n",
    "print(f'Fragment file: {os.path.basename(FRAG_FILE)} ({os.path.getsize(FRAG_FILE)/1e6:.1f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f10cf6",
   "metadata": {},
   "source": [
    "## 1. Correctness test: small file (Bonobo Adipocytes, 8.8 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ccff7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Auto-detected 1bp cut-site fragments, using 'coverage' instead of 'cutsites'\n",
      "Quantifying Adipocytes.fragments.tsv.gz (fragments) over 840,026 peaks...\n",
      "\n",
      "Time: 18.9s\n",
      "RSS: 2430 -> 2430 MB (delta: 0 MB)\n",
      "Non-zero: 258,726 / 840,026\n",
      "Sum: 719,901\n",
      "Top peaks:\n",
      "unified_768321    238\n",
      "unified_882100    156\n",
      "unified_152905    154\n",
      "unified_869106    140\n",
      "unified_153076    124\n",
      "Name: Adipocytes.fragments.tsv, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Single-file quantify on small Bonobo file\n",
    "rss_before = get_rss_mb()\n",
    "t0 = time.time()\n",
    "\n",
    "result = quantify(\n",
    "    input_file=FRAG_FILE,\n",
    "    peak_file=PEAK_FILE,\n",
    "    input_type='fragments',\n",
    "    method='cutsites',  # should auto-detect 1bp and switch to coverage\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "rss_after = get_rss_mb()\n",
    "\n",
    "print(f'\\nTime: {elapsed:.1f}s')\n",
    "print(f'RSS: {rss_before:.0f} -> {rss_after:.0f} MB (delta: {rss_after - rss_before:.0f} MB)')\n",
    "print(f'Non-zero: {(result > 0).sum():,} / {len(result):,}')\n",
    "print(f'Sum: {result.sum():,}')\n",
    "print(f'Top peaks:\\n{result.sort_values(ascending=False).head(5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74a5984a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python vs bedtools on 5000 peaks:\n",
      "  Python non-zero: 1722, sum: 5291\n",
      "  Bedtools non-zero: 1722, sum: 5291\n",
      "  Max abs difference: 0\n",
      "  Peaks with differences: 0\n",
      "  PERFECT MATCH\n"
     ]
    }
   ],
   "source": [
    "# Validation: compare Python approach vs bedtools on a small subset\n",
    "import subprocess, tempfile\n",
    "\n",
    "os.environ['PATH'] = '/cluster/project/treutlein/jjans/software/miniforge3/envs/scenicplus/bin:' + os.environ.get('PATH', '')\n",
    "\n",
    "peaks_df = load_peaks(PEAK_FILE)\n",
    "\n",
    "# Harmonize chr prefix (fragments use '1', peaks use 'chr1')\n",
    "from src.quantification import _harmonize_chr_prefix\n",
    "peaks_df = _harmonize_chr_prefix(peaks_df, FRAG_FILE, 'fragments')\n",
    "\n",
    "# Take 5000 peaks\n",
    "subset = peaks_df.head(5000)\n",
    "\n",
    "# Python approach\n",
    "peak_index = _build_peak_index(subset)\n",
    "py_counts = np.zeros(len(subset), dtype=np.int64)\n",
    "with gzip.open(FRAG_FILE, 'rt') as fh:\n",
    "    current_chrom = None\n",
    "    pos_buffer = []\n",
    "    \n",
    "    def flush():\n",
    "        global pos_buffer, current_chrom\n",
    "        if pos_buffer and current_chrom and current_chrom in peak_index:\n",
    "            entry = peak_index[current_chrom]\n",
    "            positions = np.array(pos_buffer, dtype=np.int64)\n",
    "            _count_1bp_hits(positions, entry[0], entry[1], entry[2], py_counts)\n",
    "        pos_buffer = []\n",
    "    \n",
    "    for line in fh:\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        tab1 = line.index('\\t')\n",
    "        chrom = line[:tab1]\n",
    "        if chrom != current_chrom:\n",
    "            flush()\n",
    "            current_chrom = chrom\n",
    "        tab2 = line.index('\\t', tab1 + 1)\n",
    "        pos_buffer.append(int(line[tab1+1:tab2]))\n",
    "    flush()\n",
    "\n",
    "# Bedtools approach (ground truth)\n",
    "tmp = tempfile.mktemp(suffix='.bed')\n",
    "subset[['Chromosome','Start','End','Name']].to_csv(tmp, sep='\\t', header=False, index=False)\n",
    "cmd = f'bedtools intersect -a {tmp} -b {FRAG_FILE} -c'\n",
    "res = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "bt_counts = np.array([int(line.split('\\t')[-1]) for line in res.stdout.strip().split('\\n')])\n",
    "os.unlink(tmp)\n",
    "\n",
    "# Compare\n",
    "diff = py_counts - bt_counts\n",
    "print(f'Python vs bedtools on {len(subset)} peaks:')\n",
    "print(f'  Python non-zero: {(py_counts > 0).sum()}, sum: {py_counts.sum()}')\n",
    "print(f'  Bedtools non-zero: {(bt_counts > 0).sum()}, sum: {bt_counts.sum()}')\n",
    "print(f'  Max abs difference: {np.abs(diff).max()}')\n",
    "print(f'  Peaks with differences: {(diff != 0).sum()}')\n",
    "if (diff != 0).any():\n",
    "    mismatch_idx = np.where(diff != 0)[0]\n",
    "    for i in mismatch_idx[:10]:\n",
    "        print(f'    Peak {subset.iloc[i][\"Name\"]}: py={py_counts[i]} bt={bt_counts[i]} '\n",
    "              f'[{subset.iloc[i][\"Chromosome\"]}:{subset.iloc[i][\"Start\"]}-{subset.iloc[i][\"End\"]}]')\n",
    "else:\n",
    "    print('  PERFECT MATCH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471579a8",
   "metadata": {},
   "source": [
    "## 2. Benchmark: time vs file size (estimate full-run duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee29b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking 3 files of increasing size:\n",
      "----------------------------------------------------------------------\n",
      "  Paneth_cells.fragments.tsv.gz                          4.9 MB     6.0s  (0.8 MB/s)  RSS delta: +237 MB\n",
      "  EECs.fragments.tsv.gz                                116.0 MB    49.7s  (2.3 MB/s)  RSS delta: +132 MB\n",
      "  T_cells.fragments.tsv.gz                             347.2 MB   145.3s  (2.4 MB/s)  RSS delta: +15 MB\n",
      "\n",
      "Average processing rate: 1.85 MB/s (compressed)\n",
      "Max RSS delta: 237 MB\n"
     ]
    }
   ],
   "source": [
    "# Benchmark Human files of different sizes to establish time-per-MB\n",
    "human_files = sorted(glob.glob(os.path.join(HUMAN_FRAG_DIR, '*.tsv.gz')))\n",
    "file_sizes = {f: os.path.getsize(f) for f in human_files}\n",
    "\n",
    "# Pick 3 files: smallest, ~median, and one around 300MB\n",
    "sorted_by_size = sorted(human_files, key=lambda f: file_sizes[f])\n",
    "test_files = [\n",
    "    sorted_by_size[0],    # smallest\n",
    "    sorted_by_size[len(sorted_by_size) // 2],  # median\n",
    "    sorted_by_size[-5],   # large-ish (but not the 2.9GB monster)\n",
    "]\n",
    "\n",
    "print('Benchmarking 3 files of increasing size:')\n",
    "print('-' * 70)\n",
    "benchmarks = []\n",
    "for fpath in test_files:\n",
    "    size_mb = file_sizes[fpath] / 1e6\n",
    "    rss_before = get_rss_mb()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    r = quantify(\n",
    "        input_file=fpath,\n",
    "        peak_file=HUMAN_PEAK_FILE,\n",
    "        input_type='fragments',\n",
    "        method='cutsites',\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    rss_after = get_rss_mb()\n",
    "    rate = size_mb / elapsed if elapsed > 0 else 0\n",
    "    \n",
    "    benchmarks.append({\n",
    "        'file': os.path.basename(fpath),\n",
    "        'size_mb': size_mb,\n",
    "        'time_s': elapsed,\n",
    "        'rate_mb_s': rate,\n",
    "        'non_zero': (r > 0).sum(),\n",
    "        'total_count': r.sum(),\n",
    "        'rss_delta_mb': rss_after - rss_before,\n",
    "    })\n",
    "    print(f'  {os.path.basename(fpath):50s} '\n",
    "          f'{size_mb:>7.1f} MB  {elapsed:>6.1f}s  ({rate:.1f} MB/s)  '\n",
    "          f'RSS delta: {rss_after - rss_before:+.0f} MB')\n",
    "\n",
    "bench_df = pd.DataFrame(benchmarks)\n",
    "avg_rate = bench_df['rate_mb_s'].mean()\n",
    "print(f'\\nAverage processing rate: {avg_rate:.2f} MB/s (compressed)')\n",
    "print(f'Max RSS delta: {bench_df[\"rss_delta_mb\"].max():.0f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31ded139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using benchmark rate: 1.88 MB/s (compressed)\n",
      "Species         Files   Total MB  Est. serial  Est. 4 workers\n",
      "-----------------------------------------------------------------\n",
      "Bonobo             35       2390       1270s (21m)        318s (5m)\n",
      "Chimpanzee         32       3761       1998s (33m)        499s (8m)\n",
      "Gorilla            33       3320       1764s (29m)        441s (7m)\n",
      "Macaque            32       1798        955s (16m)        239s (4m)\n",
      "Marmoset           32       4356       2314s (39m)        579s (10m)\n",
      "Human              32       9108       4839s (81m)       1210s (20m)\n",
      "-----------------------------------------------------------------\n",
      "TOTAL                      24734      13140s (219m)       3285s (55m)\n",
      "\n",
      "RAM per worker: ~50 MB (peak index + Python overhead)\n",
      "RAM with 4 workers: ~200 MB\n"
     ]
    }
   ],
   "source": [
    "# Estimate total time for all species\n",
    "species_dirs = {\n",
    "    'Bonobo': f'{BASE}/fragment_files/Bonobo',\n",
    "    'Chimpanzee': f'{BASE}/fragment_files/Chimpanzee',\n",
    "    'Gorilla': f'{BASE}/fragment_files/Gorilla',\n",
    "    'Macaque': f'{BASE}/fragment_files/Macaque',\n",
    "    'Marmoset': f'{BASE}/fragment_files/Marmoset',\n",
    "    'Human': f'{BASE}/fragment_files/Human',\n",
    "}\n",
    "\n",
    "print(f'Using benchmark rate: {avg_rate:.2f} MB/s (compressed)')\n",
    "print(f'{\"Species\":<15} {\"Files\":>5} {\"Total MB\":>10} {\"Est. serial\":>12} {\"Est. 4 workers\":>15}')\n",
    "print('-' * 65)\n",
    "\n",
    "grand_total_mb = 0\n",
    "for sp, sdir in species_dirs.items():\n",
    "    files = glob.glob(os.path.join(sdir, '*.tsv.gz'))\n",
    "    total_mb = sum(os.path.getsize(f) for f in files) / 1e6\n",
    "    grand_total_mb += total_mb\n",
    "    est_serial = total_mb / avg_rate\n",
    "    est_parallel = total_mb / avg_rate / 4  # 4 workers\n",
    "    print(f'{sp:<15} {len(files):>5} {total_mb:>10.0f} {est_serial:>10.0f}s ({est_serial/60:.0f}m)'\n",
    "          f' {est_parallel:>10.0f}s ({est_parallel/60:.0f}m)')\n",
    "\n",
    "print('-' * 65)\n",
    "print(f'{\"TOTAL\":<15} {\"\":>5} {grand_total_mb:>10.0f} '\n",
    "      f'{grand_total_mb/avg_rate:>10.0f}s ({grand_total_mb/avg_rate/60:.0f}m)'\n",
    "      f' {grand_total_mb/avg_rate/4:>10.0f}s ({grand_total_mb/avg_rate/4/60:.0f}m)')\n",
    "print(f'\\nRAM per worker: ~{bench_df[\"rss_delta_mb\"].max() + 50:.0f} MB (peak index + Python overhead)')\n",
    "print(f'RAM with 4 workers: ~{4 * (bench_df[\"rss_delta_mb\"].max() + 50):.0f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fa26e",
   "metadata": {},
   "source": [
    "## 3. Test parallelization with quantify_matrix (small subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c212c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parallelization with 4 files:\n",
      "  Paneth_cells.fragments.tsv.gz                      4.9 MB\n",
      "  ILCs.fragments.tsv.gz                              7.2 MB\n",
      "  Monocytes.fragments.tsv.gz                         8.3 MB\n",
      "  Adipocytes.fragments.tsv.gz                        9.2 MB\n",
      "\n",
      "  n_workers=1: 14.7s  (speedup: 1.0x)  RSS delta: +0 MB\n",
      "\n",
      "  n_workers=2: 17.1s  (speedup: 0.9x)  RSS delta: +0 MB\n",
      "\n",
      "  n_workers=4: 21.5s  (speedup: 0.7x)  RSS delta: +0 MB\n",
      "  n_workers=2 matches serial: True\n",
      "  n_workers=4 matches serial: True\n",
      "\n",
      "Matrix shape: (887532, 4)\n",
      "Note: no speedup expected on login node due to shared filesystem I/O contention.\n",
      "On a compute node with dedicated I/O, 4 workers should give ~2-3x speedup.\n"
     ]
    }
   ],
   "source": [
    "# Test quantify_matrix with 4 small files (serial vs 2-worker vs 4-worker)\n",
    "small_files = sorted_by_size[:4]\n",
    "print(f'Testing parallelization with {len(small_files)} files:')\n",
    "for f in small_files:\n",
    "    print(f'  {os.path.basename(f):50s} {file_sizes[f]/1e6:.1f} MB')\n",
    "\n",
    "results = {}\n",
    "for n_w in [1, 2, 4]:\n",
    "    rss_before = get_rss_mb()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    mat = quantify_matrix(\n",
    "        input_files=small_files,\n",
    "        peak_file=HUMAN_PEAK_FILE,\n",
    "        input_type='fragments',\n",
    "        method='cutsites',\n",
    "        n_workers=n_w,\n",
    "        name_pattern=r'\\.fragments.*$',\n",
    "        name_replacement='',\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - t0\n",
    "    rss_after = get_rss_mb()\n",
    "    results[n_w] = {'time': elapsed, 'rss_delta': rss_after - rss_before, 'mat': mat}\n",
    "    print(f'\\n  n_workers={n_w}: {elapsed:.1f}s  '\n",
    "          f'(speedup: {results[1][\"time\"]/elapsed:.1f}x)  '\n",
    "          f'RSS delta: {rss_after - rss_before:+.0f} MB')\n",
    "\n",
    "# Verify all produce identical results\n",
    "ref = results[1]['mat']\n",
    "for n_w in [2, 4]:\n",
    "    match = (results[n_w]['mat'] == ref).all().all()\n",
    "    print(f'  n_workers={n_w} matches serial: {match}')\n",
    "\n",
    "print(f'\\nMatrix shape: {ref.shape}')\n",
    "print(f'Note: no speedup expected on login node due to shared filesystem I/O contention.')\n",
    "print(f'On a compute node with dedicated I/O, 4 workers should give ~2-3x speedup.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f9151",
   "metadata": {},
   "source": [
    "## 4. RAM profile: measure peak RSS during quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f803eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak index memory breakdown:\n",
      "  Peaks: 887,532\n",
      "  Chromosomes in index: 63\n",
      "  Index arrays: 21.3 MB\n",
      "  Counts array: 7.1 MB\n",
      "  Position buffer (500K): 4.0 MB\n",
      "  Total per-worker data: 32.4 MB\n",
      "  Python interpreter overhead: ~50 MB\n",
      "  Estimated per-worker: ~82 MB\n",
      "  Estimated 4 workers: ~330 MB\n",
      "  Estimated 8 workers: ~659 MB\n",
      "\n",
      "  Compare: bedtools without -sorted loaded the ENTIRE fragment file into RAM\n",
      "  TA_cells.tsv.gz alone = ~35 GB uncompressed, x4 workers = 140 GB\n"
     ]
    }
   ],
   "source": [
    "# RAM analysis\n",
    "# Peak index: 888K peaks x 3 arrays (starts, ends, row_indices) x 8 bytes = ~20 MB\n",
    "# Counts array: 888K x 8 bytes = ~7 MB\n",
    "# Position buffer: 500K x 8 bytes = ~4 MB\n",
    "# Per-worker overhead: ~31 MB + Python interpreter (~50 MB) = ~80 MB\n",
    "\n",
    "peaks = load_peaks(HUMAN_PEAK_FILE)\n",
    "peak_index = _build_peak_index(peaks)\n",
    "\n",
    "# Measure actual peak index memory\n",
    "index_bytes = sum(\n",
    "    s.nbytes + e.nbytes + r.nbytes\n",
    "    for s, e, r in peak_index.values()\n",
    ")\n",
    "counts_bytes = len(peaks) * 8  # int64\n",
    "\n",
    "print(f'Peak index memory breakdown:')\n",
    "print(f'  Peaks: {len(peaks):,}')\n",
    "print(f'  Chromosomes in index: {len(peak_index)}')\n",
    "print(f'  Index arrays: {index_bytes / 1e6:.1f} MB')\n",
    "print(f'  Counts array: {counts_bytes / 1e6:.1f} MB')\n",
    "print(f'  Position buffer (500K): {500_000 * 8 / 1e6:.1f} MB')\n",
    "print(f'  Total per-worker data: {(index_bytes + counts_bytes + 500_000*8) / 1e6:.1f} MB')\n",
    "print(f'  Python interpreter overhead: ~50 MB')\n",
    "print(f'  Estimated per-worker: ~{(index_bytes + counts_bytes + 500_000*8) / 1e6 + 50:.0f} MB')\n",
    "print(f'  Estimated 4 workers: ~{4 * ((index_bytes + counts_bytes + 500_000*8) / 1e6 + 50):.0f} MB')\n",
    "print(f'  Estimated 8 workers: ~{8 * ((index_bytes + counts_bytes + 500_000*8) / 1e6 + 50):.0f} MB')\n",
    "print(f'\\n  Compare: bedtools without -sorted loaded the ENTIRE fragment file into RAM')\n",
    "print(f'  TA_cells.tsv.gz alone = ~35 GB uncompressed, x4 workers = 140 GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1eb227",
   "metadata": {},
   "source": [
    "## 5. Polars vs Pandas CSV reader benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3573f9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars 1.38.1 available\n",
      "Test file: EECs.fragments.tsv.gz (116 MB)\n",
      "Peaks: 887,532\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3463150/2545815895.py:65: DeprecationWarning: the argument `dtypes` for `read_csv_batched` is deprecated. It was renamed to `schema_overrides` in version 0.20.31.\n",
      "  reader = pl.read_csv_batched(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polars batched not available for .gz: projections contained duplicate output name 'literal'. It's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.\n",
      "\n",
      "Resolved plan until failure:\n",
      "\n",
      "\t---> FAILED HERE RESOLVING 'select' <---\n",
      "Csv SCAN [/cluster/project/treutlein/USERS/jjans/analysis/adult_intestine/peaks/fragment_files/Human/EECs.fragments.tsv.gz]\n",
      "PROJECT */8 COLUMNS\n",
      "  pandas chunked              40.2s  (2.9 MB/s)  sum=12,524,461  match=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3463150/2545815895.py:46: DeprecationWarning: the argument `dtypes` for `read_csv` is deprecated. It was renamed to `schema_overrides` in version 0.20.31.\n",
      "  df = pl.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  polars full read            18.5s  (6.3 MB/s)  sum=12,524,461  match=True\n",
      "\n",
      "Conclusion: polars is faster\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import polars as pl\n",
    "    print(f'Polars {pl.__version__} available')\n",
    "except ImportError:\n",
    "    pl = None\n",
    "    print('Polars not installed -- run: pip install polars')\n",
    "\n",
    "if pl is not None:\n",
    "    from src.quantification import _build_peak_index, _count_1bp_hits, _harmonize_chr_prefix\n",
    "\n",
    "    # Use the medium Human file for a meaningful comparison\n",
    "    test_file = sorted_by_size[len(sorted_by_size) // 2]  # EECs, 116 MB\n",
    "    peaks = load_peaks(HUMAN_PEAK_FILE)\n",
    "    peaks = _harmonize_chr_prefix(peaks, test_file, 'fragments')\n",
    "    pi = _build_peak_index(peaks)\n",
    "    n_peaks = len(peaks)\n",
    "\n",
    "    print(f'Test file: {os.path.basename(test_file)} ({file_sizes[test_file]/1e6:.0f} MB)')\n",
    "    print(f'Peaks: {n_peaks:,}')\n",
    "    print('-' * 70)\n",
    "\n",
    "    # --- Method 1: current pandas chunked approach ---\n",
    "    def count_pandas(fpath, peak_index, n_peaks):\n",
    "        counts = np.zeros(n_peaks, dtype=np.int64)\n",
    "        reader = pd.read_csv(\n",
    "            fpath, sep='\\t', header=None,\n",
    "            usecols=[0, 1], names=['c', 's'],\n",
    "            dtype={'c': str, 's': np.int64},\n",
    "            comment='#', engine='c', chunksize=500_000,\n",
    "        )\n",
    "        for chunk in reader:\n",
    "            chroms = chunk['c'].values\n",
    "            starts = chunk['s'].values\n",
    "            for ch in np.unique(chroms):\n",
    "                entry = peak_index.get(ch)\n",
    "                if entry is None:\n",
    "                    continue\n",
    "                positions = starts[chroms == ch]\n",
    "                _count_1bp_hits(positions, entry[0], entry[1], entry[2], counts)\n",
    "        return counts\n",
    "\n",
    "    # --- Method 2: polars scan_csv (lazy, streaming) ---\n",
    "    def count_polars_lazy(fpath, peak_index, n_peaks):\n",
    "        counts = np.zeros(n_peaks, dtype=np.int64)\n",
    "        # Polars can read gzipped TSV natively; use scan_csv for streaming\n",
    "        df = pl.read_csv(\n",
    "            fpath, separator='\\t', has_header=False,\n",
    "            columns=[0, 1], new_columns=['c', 's'],\n",
    "            dtypes={'c': pl.Utf8, 's': pl.Int64},\n",
    "            comment_prefix='#',\n",
    "        )\n",
    "        # Group by chromosome -- polars partition_by is very fast\n",
    "        for ch, group in df.group_by('c'):\n",
    "            ch_name = ch[0]  # group_by returns tuple keys\n",
    "            entry = peak_index.get(ch_name)\n",
    "            if entry is None:\n",
    "                continue\n",
    "            positions = group['s'].to_numpy()\n",
    "            _count_1bp_hits(positions, entry[0], entry[1], entry[2], counts)\n",
    "        return counts\n",
    "\n",
    "    # --- Method 3: polars with batched reading ---\n",
    "    def count_polars_batched(fpath, peak_index, n_peaks):\n",
    "        counts = np.zeros(n_peaks, dtype=np.int64)\n",
    "        reader = pl.read_csv_batched(\n",
    "            fpath, separator='\\t', has_header=False,\n",
    "            columns=[0, 1], new_columns=['c', 's'],\n",
    "            dtypes={'c': pl.Utf8, 's': pl.Int64},\n",
    "            comment_prefix='#',\n",
    "            batch_size=500_000,\n",
    "        )\n",
    "        while True:\n",
    "            batches = reader.next_batches(1)\n",
    "            if not batches:\n",
    "                break\n",
    "            chunk = batches[0]\n",
    "            chroms = chunk['c'].to_numpy()\n",
    "            starts = chunk['s'].to_numpy()\n",
    "            for ch in np.unique(chroms):\n",
    "                entry = peak_index.get(ch)\n",
    "                if entry is None:\n",
    "                    continue\n",
    "                positions = starts[chroms == ch]\n",
    "                _count_1bp_hits(positions, entry[0], entry[1], entry[2], counts)\n",
    "        return counts\n",
    "\n",
    "    # Benchmark all three\n",
    "    methods = {\n",
    "        'pandas chunked': lambda: count_pandas(test_file, pi, n_peaks),\n",
    "        'polars full read': lambda: count_polars_lazy(test_file, pi, n_peaks),\n",
    "    }\n",
    "    # batched reader may not support gzip in all versions; try it\n",
    "    try:\n",
    "        _ = count_polars_batched(test_file, pi, n_peaks)\n",
    "        methods['polars batched'] = lambda: count_polars_batched(test_file, pi, n_peaks)\n",
    "    except Exception as e:\n",
    "        print(f'polars batched not available for .gz: {e}')\n",
    "\n",
    "    ref = None\n",
    "    for name, fn in methods.items():\n",
    "        t0 = time.time()\n",
    "        result = fn()\n",
    "        elapsed = time.time() - t0\n",
    "        rate = file_sizes[test_file] / 1e6 / elapsed\n",
    "        if ref is None:\n",
    "            ref = result\n",
    "            match = True\n",
    "        else:\n",
    "            match = np.array_equal(ref, result)\n",
    "        print(f'  {name:<25} {elapsed:>6.1f}s  ({rate:.1f} MB/s)  '\n",
    "              f'sum={result.sum():,}  match={match}')\n",
    "\n",
    "    print(f'\\nConclusion: polars is {\"faster\" if methods else \"N/A\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c666b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars available: True\n",
      "Test file: EECs.fragments.tsv.gz (116 MB)\n",
      "\n",
      "Polars path: 19.9s\n",
      "Sum: 12,524,461\n",
      "Non-zero: 733,520\n",
      "Matches previous ref: True\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import time\n",
    "from src import quantification as Q\n",
    "importlib.reload(Q)\n",
    "\n",
    "print(f\"Polars available: {Q._HAS_POLARS}\")\n",
    "\n",
    "# Use the human fragment file that was benchmarked\n",
    "BASE = '/cluster/project/treutlein/USERS/jjans/analysis/adult_intestine/peaks'\n",
    "HUMAN_FRAG_DIR = f'{BASE}/fragment_files/Human'\n",
    "test_file = os.path.join(HUMAN_FRAG_DIR, 'EECs.fragments.tsv.gz')\n",
    "print(f\"Test file: {os.path.basename(test_file)} ({os.path.getsize(test_file)/1e6:.0f} MB)\")\n",
    "\n",
    "# Run via the updated module (should use polars automatically)\n",
    "t0 = time.time()\n",
    "result_polars = Q._count_fragments_coverage(test_file, peak_index=pi, n_peaks=n_peaks)\n",
    "t_polars = time.time() - t0\n",
    "\n",
    "# Compare against the known reference from the benchmark (sum=12,524,461)\n",
    "print(f\"\\nPolars path: {t_polars:.1f}s\")\n",
    "print(f\"Sum: {result_polars.sum():,}\")\n",
    "print(f\"Non-zero: {(result_polars > 0).sum():,}\")\n",
    "print(f\"Matches previous ref: {np.array_equal(result_polars, ref)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
